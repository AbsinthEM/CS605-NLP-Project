{"cells":[{"cell_type":"markdown","source":["# 3-Class BiLSTM Sentiment Classifier on Yelp Reviews\n","\n","**High-level summary:**  \n","An end-to-end PyTorch pipeline that loads parquet review data, maps star ratings to 3 sentiment classes, preprocesses text, builds a vocabulary with GloVe embeddings, constructs a balanced DataLoader (or weighted sampler), defines a BiLSTM classifier, trains with class-imbalance handling, evaluates with detailed metrics, saves the model, and provides an inference utility."],"metadata":{"id":"9W9ClGLyFgah"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25665,"status":"ok","timestamp":1750313091753,"user":{"displayName":"Kindness Matters","userId":"04525017512802736418"},"user_tz":-480},"id":"LjqAXWWnCZr9","outputId":"90d14b0c-08fe-4437-a02a-3e979a5ac896"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# prompt: connect google drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# prompt: load current directory\n","\n","import os\n","\n","os.chdir('/content/drive/My Drive/CS605-NLP-Project')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mUAMIK1D_Ys"},"outputs":[],"source":["# Install\n","#!pip install --upgrade numpy gensim --no-cache-dir\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IDvyCWXQDXg0"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{"id":"H5XmBWJJIntR"},"source":["Reads the Yelp train/test sets (in Parquet format) into pandas.\n","\n","Reads your USS reviews CSV for later inference.\n","\n","Prints out the number of rows/columns and shows the first few records of each."]},{"cell_type":"code","source":["import re\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n","from tqdm import tqdm\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import classification_report, confusion_matrix, f1_score\n","\n","# 1. Load your data (update file paths as needed)\n","train_df = pd.read_parquet(\"datastore/train-00000-of-00001.parquet\")\n","test_df  = pd.read_parquet(\"datastore/test-00000-of-00001.parquet\")\n","\n","# 1. LOAD & MAP LABELS\n","def map_label(star):\n","    if star in [0, 1]:\n","        return 0   # Negative\n","    elif star == 2:\n","        return 1   # Neutral\n","    else:\n","        return 2   # Positive\n","\n","train_df['label'] = train_df['label'].apply(map_label)\n","test_df['label']  = test_df['label'].apply(map_label)\n","\n","# Print class distribution\n","print(\"Training set class distribution:\")\n","print(train_df['label'].value_counts().sort_index())\n","print(\"\\nTest set class distribution:\")\n","print(test_df['label'].value_counts().sort_index())\n","\n","# Calculate class weights for imbalanced dataset\n","train_labels = train_df['label'].values\n","class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n","print(f\"\\nClass weights: {class_weights}\")\n","\n","# 2. TEXT PREPROCESSING\n","def preprocess_text(text):\n","    text = text.lower()\n","    text = re.sub(r'[^a-z0-9\\s]', '', text)\n","    return text.split()\n","\n","# 3. BUILD VOCAB\n","MAX_VOCAB_SIZE = 20000\n","PAD, UNK = '<PAD>', '<UNK>'\n","token_counts = {}\n","for txt in tqdm(train_df['text'], desc='Counting tokens'):\n","    for tok in preprocess_text(txt):\n","        token_counts[tok] = token_counts.get(tok, 0) + 1\n","\n","# keep top‚ÄêMAX_VOCAB_SIZE\n","most_common = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n","itos = [PAD, UNK] + [tok for tok, _ in most_common[:MAX_VOCAB_SIZE-2]]\n","stoi = {tok: i for i, tok in enumerate(itos)}\n","\n","# 4. LOAD GloVe\n","EMBED_DIM = 100\n","GLOVE_PATH = 'datastore/glove.6B.100d.txt'  # adjust if needed\n","emb_index = {}\n","try:\n","    with open(GLOVE_PATH, encoding='utf8') as f:\n","        for line in f:\n","            parts = line.split()\n","            word  = parts[0]\n","            vec   = np.array(parts[1:], dtype='float32')\n","            emb_index[word] = vec\n","    print(f\"Loaded {len(emb_index)} GloVe embeddings\")\n","except FileNotFoundError:\n","    print(f\"GloVe file not found at {GLOVE_PATH}. Using random embeddings.\")\n","    emb_index = {}\n","\n","vocab_size = len(itos)\n","emb_matrix = np.zeros((vocab_size, EMBED_DIM))\n","for i, tok in enumerate(itos):\n","    emb_matrix[i] = emb_index.get(tok, np.random.normal(scale=0.6, size=(EMBED_DIM,)))\n","\n","# 5. DATASET & DATALOADER\n","MAX_LEN = 200\n","BATCH_SIZE = 128\n","\n","class YelpDataset(Dataset):\n","    def __init__(self, df):\n","        self.texts  = df['text'].tolist()\n","        self.labels = df['label'].tolist()\n","    def __len__(self):\n","        return len(self.labels)\n","    def __getitem__(self, idx):\n","        toks = preprocess_text(self.texts[idx])\n","        seq = [stoi.get(t, stoi[UNK]) for t in toks]\n","        if len(seq) < MAX_LEN:\n","            seq += [stoi[PAD]] * (MAX_LEN - len(seq))\n","        else:\n","            seq = seq[:MAX_LEN]\n","        return torch.tensor(seq, dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n","\n","train_ds = YelpDataset(train_df)\n","test_ds  = YelpDataset(test_df)\n","\n","# Create balanced sampler for training\n","def create_balanced_sampler(labels):\n","    \"\"\"Create weighted sampler to handle class imbalance\"\"\"\n","    class_counts = np.bincount(labels)\n","    class_weights = 1.0 / class_counts\n","    sample_weights = [class_weights[label] for label in labels]\n","\n","    sampler = WeightedRandomSampler(\n","        weights=sample_weights,\n","        num_samples=len(sample_weights),\n","        replacement=True\n","    )\n","    return sampler\n","\n","# Option 1: Use balanced sampler\n","use_balanced_sampler = False  # Set to True to use balanced sampling instead of weighted loss\n","\n","if use_balanced_sampler:\n","    train_sampler = create_balanced_sampler(train_labels)\n","    train_ld = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler)\n","    print(\"Using balanced sampler\")\n","else:\n","    train_ld = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n","    print(\"Using weighted loss function\")\n","\n","test_ld = DataLoader(test_ds, batch_size=BATCH_SIZE)\n","\n","# 6. MODEL\n","class BiLSTMClassifier(nn.Module):\n","    def __init__(self, emb_matrix, hidden_dim, n_classes, n_layers=1, dropout=0.5):\n","        super().__init__()\n","        vocab_sz, emb_dim = emb_matrix.shape\n","        self.embedding = nn.Embedding(vocab_sz, emb_dim, padding_idx=stoi[PAD])\n","        self.embedding.weight.data.copy_(torch.from_numpy(emb_matrix))\n","        self.embedding.weight.requires_grad = False\n","        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=n_layers,\n","                            bidirectional=True, batch_first=True,\n","                            dropout=dropout if n_layers>1 else 0)\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(hidden_dim*2, n_classes)\n","\n","    def forward(self, x):\n","        x_emb = self.embedding(x)\n","        _, (h_n, _) = self.lstm(x_emb)\n","        h_f = h_n[-2]  # forward final\n","        h_b = h_n[-1]  # backward final\n","        h   = torch.cat([h_f, h_b], dim=1)\n","        return self.fc(self.dropout(h))\n","\n","device      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","hidden_dim  = 64\n","n_classes   = 3\n","model       = BiLSTMClassifier(emb_matrix, hidden_dim, n_classes).to(device)\n","\n","# Use weighted loss function to handle class imbalance\n","if not use_balanced_sampler:\n","    class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n","    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n","    print(f\"Using weighted CrossEntropyLoss with weights: {class_weights}\")\n","else:\n","    criterion = nn.CrossEntropyLoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","\n","# 7. TRAIN\n","EPOCHS = 5\n","for epoch in range(1, EPOCHS+1):\n","    model.train()\n","    total_loss, total_acc = 0, 0\n","    for texts, labels in tqdm(train_ld, desc=f'Epoch {epoch}/{EPOCHS}'):\n","        texts, labels = texts.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        out = model(texts)\n","        loss = criterion(out, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item() * texts.size(0)\n","        total_acc  += (out.argmax(1)==labels).sum().item()\n","    avg_l = total_loss/len(train_ds)\n","    avg_a = total_acc/len(train_ds)\n","    print(f\"Epoch {epoch}: loss={avg_l:.4f}, acc={avg_a:.4f}\")\n","\n","# 8. DETAILED EVALUATION WITH IMBALANCE-AWARE METRICS\n","def evaluate_model_detailed(model, test_loader, device):\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    test_loss = 0\n","\n","    with torch.no_grad():\n","        for texts, labels in tqdm(test_ld, desc=\"Evaluating\"):\n","            texts, labels = texts.to(device), labels.to(device)\n","            out = model(texts)\n","            test_loss += criterion(out, labels).item() * texts.size(0)\n","\n","            preds = out.argmax(1).cpu().numpy()\n","            labels_np = labels.cpu().numpy()\n","\n","            all_preds.extend(preds)\n","            all_labels.extend(labels_np)\n","\n","    test_loss /= len(test_ds)\n","    overall_acc = sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_labels)\n","\n","    # Calculate balanced metrics\n","    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n","    weighted_f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","    print(f\"\\n{'='*50}\")\n","    print(f\"DETAILED EVALUATION RESULTS\")\n","    print(f\"{'='*50}\")\n","    print(f\"Test Loss: {test_loss:.4f}\")\n","    print(f\"Overall Accuracy: {overall_acc:.4f}\")\n","    print(f\"Macro F1-Score: {macro_f1:.4f}\")\n","    print(f\"Weighted F1-Score: {weighted_f1:.4f}\")\n","\n","    print(f\"\\nClassification Report:\")\n","    print(classification_report(all_labels, all_preds,\n","                              target_names=['Negative', 'Neutral', 'Positive'],\n","                              digits=4))\n","\n","    print(f\"\\nConfusion Matrix:\")\n","    cm = confusion_matrix(all_labels, all_preds)\n","    print(\"       Predicted\")\n","    print(\"       Neg  Neu  Pos\")\n","    for i, row in enumerate(cm):\n","        class_name = ['Neg', 'Neu', 'Pos'][i]\n","        print(f\"{class_name:>6} {row[0]:4d} {row[1]:4d} {row[2]:4d}\")\n","\n","    return overall_acc, macro_f1, weighted_f1\n","\n","# Run detailed evaluation\n","evaluate_model_detailed(model, test_ld, device)\n","\n","# 9. SAVE MODEL\n","torch.save({\n","    'model_state_dict': model.state_dict(),\n","    'vocab': {'itos': itos, 'stoi': stoi},\n","    'class_weights': class_weights,\n","    'config': {\n","        'hidden_dim': hidden_dim,\n","        'n_classes': n_classes,\n","        'max_len': MAX_LEN,\n","        'embed_dim': EMBED_DIM\n","    }\n","}, 'model/3class_bilstm_yelp.pth')\n","print(\"\\nModel saved as 'model/3class_bilstm_yelp.pth'\")\n","\n","# 10. EXAMPLE PREDICTIONS\n","def predict_text(model, text, stoi, device, max_len=200):\n","    model.eval()\n","    tokens = preprocess_text(text)\n","    seq = [stoi.get(t, stoi[UNK]) for t in tokens]\n","    if len(seq) < max_len:\n","        seq += [stoi[PAD]] * (max_len - len(seq))\n","    else:\n","        seq = seq[:max_len]\n","\n","    input_tensor = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        output = model(input_tensor)\n","        probabilities = torch.softmax(output, dim=1)\n","        predicted_class = output.argmax(dim=1).item()\n","        confidence = probabilities[0][predicted_class].item()\n","\n","    class_names = ['Negative', 'Neutral', 'Positive']\n","    return class_names[predicted_class], confidence, probabilities[0].cpu().numpy()\n","\n","print(f\"\\n{'='*50}\")\n","print(\"EXAMPLE PREDICTIONS\")\n","print(f\"{'='*50}\")\n","sample_texts = [\n","    \"This restaurant is absolutely amazing! Great food and service.\",\n","    \"The food was okay, nothing special but not bad either.\",\n","    \"Terrible experience, worst meal I've ever had.\"\n","]\n","\n","for text in sample_texts:\n","    sentiment, confidence, probs = predict_text(model, text, stoi, device)\n","    print(f\"\\nText: {text}\")\n","    print(f\"Predicted: {sentiment} (confidence: {confidence:.3f})\")\n","    print(f\"Probabilities - Negative: {probs[0]:.3f}, Neutral: {probs[1]:.3f}, Positive: {probs[2]:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O6QL5honGcJG","executionInfo":{"status":"ok","timestamp":1750313555966,"user_tz":-480,"elapsed":462549,"user":{"displayName":"Kindness Matters","userId":"04525017512802736418"}},"outputId":"5cf93880-ee0a-4dc8-a913-93bce1e8b389"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training set class distribution:\n","label\n","0    260000\n","1    130000\n","2    260000\n","Name: count, dtype: int64\n","\n","Test set class distribution:\n","label\n","0    20000\n","1    10000\n","2    20000\n","Name: count, dtype: int64\n","\n","Class weights: [0.83333333 1.66666667 0.83333333]\n"]},{"output_type":"stream","name":"stderr","text":["Counting tokens: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 650000/650000 [00:33<00:00, 19380.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loaded 400000 GloVe embeddings\n","Using weighted loss function\n","Using weighted CrossEntropyLoss with weights: [0.83333333 1.66666667 0.83333333]\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5079/5079 [01:18<00:00, 64.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: loss=0.7561, acc=0.6759\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5079/5079 [01:18<00:00, 64.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: loss=0.6356, acc=0.7375\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5079/5079 [01:18<00:00, 64.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3: loss=0.6039, acc=0.7528\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5079/5079 [01:17<00:00, 65.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: loss=0.5855, acc=0.7616\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5079/5079 [01:17<00:00, 65.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5: loss=0.5730, acc=0.7673\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:04<00:00, 79.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","==================================================\n","DETAILED EVALUATION RESULTS\n","==================================================\n","Test Loss: 0.5551\n","Overall Accuracy: 0.7730\n","Macro F1-Score: 0.7479\n","Weighted F1-Score: 0.7840\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative     0.8875    0.7871    0.8343     20000\n","     Neutral     0.4847    0.6830    0.5670     10000\n","    Positive     0.8847    0.8038    0.8423     20000\n","\n","    accuracy                         0.7730     50000\n","   macro avg     0.7523    0.7580    0.7479     50000\n","weighted avg     0.8058    0.7730    0.7840     50000\n","\n","\n","Confusion Matrix:\n","       Predicted\n","       Neg  Neu  Pos\n","   Neg 15742 3790  468\n","   Neu 1543 6830 1627\n","   Pos  452 3472 16076\n","\n","Model saved as 'model/3class_bilstm_yelp.pth'\n","\n","==================================================\n","EXAMPLE PREDICTIONS\n","==================================================\n","\n","Text: This restaurant is absolutely amazing! Great food and service.\n","Predicted: Positive (confidence: 0.993)\n","Probabilities - Negative: 0.000, Neutral: 0.007, Positive: 0.993\n","\n","Text: The food was okay, nothing special but not bad either.\n","Predicted: Neutral (confidence: 0.789)\n","Probabilities - Negative: 0.193, Neutral: 0.789, Positive: 0.018\n","\n","Text: Terrible experience, worst meal I've ever had.\n","Predicted: Negative (confidence: 0.994)\n","Probabilities - Negative: 0.994, Neutral: 0.006, Positive: 0.000\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyPluhMXrEw1i9eHBCruF+z+"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}