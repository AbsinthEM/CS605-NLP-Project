{"cells":[{"cell_type":"markdown","source":["# Adaptive Mixture-of-Experts for Cross-Domain Sentiment Classification\n","\n","**High-level summary:**  \n","Routes each Google review dynamically to the best-suited model (Transformer, BiLSTM, or Logistic Regression) based on its length, sentiment indicators, detail level, and complexity, then ensembles their outputs with adaptive weights.\n","\n","**Table 1: Base Routing Rules**\n","\n","| Condition                                                         | Transformer | BiLSTM | Logistic | Primary Model | Notes                                                                                           |\n","|-------------------------------------------------------------------|:-----------:|:------:|:--------:|:-------------:|-------------------------------------------------------------------------------------------------|\n","| `length < 15`                                                     |   0.25      |  0.45  |   0.30   | logistic      | Short Google reviews (< 15 words) – favor simpler models; BiLSTM + Logistic for simple patterns |\n","| `length > 50`                                                     |   0.60      |  0.25  |   0.15   | bilstm        | Very detailed reviews (> 50 words) – favor Transformer; Transformer excels at long sequences     |\n","| `complexity_score > 0.05` **OR** `detail_level > 0.1`             |   0.55      |  0.30  |   0.15   | transformer   | High complexity or detailed mentions – Transformer for complex reasoning                        |\n","| `sentiment_clarity > 0.15`                                        |   0.40      |  0.40  |   0.20   | logistic      | Clear emotional sentiment – balanced approach; Transformer + BiLSTM for sentiment                |\n","| `15 ≤ length ≤ 30`                                                |   0.45      |  0.35  |   0.20   | bilstm        | Medium length, standard reviews – slight Transformer preference; Standard routing               |\n","| **Else** (ambiguous / neutral)                                    |   0.40      |  0.35  |   0.25   | transformer   | Ambiguous or neutral – favor ensemble diversity; More balanced for uncertain cases               |\n","\n"],"metadata":{"id":"b25chHrnIifR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"o9xMN5cggPdJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750352668617,"user_tz":-480,"elapsed":54263,"user":{"displayName":"Kindness Matters","userId":"04525017512802736418"}},"outputId":"ec656a10-5055-4e24-b1d5-3e5b01f80344"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# prompt: connect google drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# prompt: load current directory\n","\n","import os\n","\n","os.chdir('/content/drive/My Drive/CS605-NLP-Project')"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","import re\n","import joblib\n","import time\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import math\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","class AdaptiveMixtureOfExperts:\n","    \"\"\"\n","    Adaptive routing system for Yelp-trained models on Google reviews\n","    Routes different review types to specialized models\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.models = {}\n","        self.routing_stats = {\n","            'transformer': 0,\n","            'bilstm': 0,\n","            'logistic': 0\n","        }\n","\n","        print(f\"🧠 Adaptive MoE initialized on {self.device}\")\n","\n","    def analyze_review_characteristics(self, text):\n","        \"\"\"\n","        Analyze Google review characteristics for intelligent routing\n","        \"\"\"\n","        if pd.isna(text) or text == \"\":\n","            return {\n","                'length': 0,\n","                'sentiment_clarity': 0.5,\n","                'detail_level': 0,\n","                'complexity_score': 0,\n","                'platform_style': 'simple'\n","            }\n","\n","        text = str(text).lower()\n","        words = text.split()\n","\n","        # 1. Text length analysis\n","        length = len(words)\n","\n","        # 2. Sentiment strength indicators\n","        positive_words = ['amazing', 'excellent', 'fantastic', 'love', 'great', 'awesome', 'perfect', 'wonderful']\n","        negative_words = ['terrible', 'awful', 'horrible', 'hate', 'worst', 'disgusting', 'bad', 'disappointing']\n","        neutral_words = ['okay', 'average', 'decent', 'fine', 'normal', 'standard']\n","\n","        pos_count = sum(1 for word in positive_words if word in text)\n","        neg_count = sum(1 for word in negative_words if word in text)\n","        neu_count = sum(1 for word in neutral_words if word in text)\n","\n","        total_sentiment = pos_count + neg_count + neu_count\n","        sentiment_clarity = total_sentiment / max(length, 1) if length > 0 else 0\n","\n","        # 3. Detail level (specific mentions)\n","        detail_indicators = ['staff', 'service', 'food', 'price', 'atmosphere', 'location', 'time', 'experience']\n","        detail_count = sum(1 for indicator in detail_indicators if indicator in text)\n","        detail_level = detail_count / max(length, 1) if length > 0 else 0\n","\n","        # 4. Complexity indicators\n","        complex_patterns = [\n","            r'but\\s+', r'however\\s+', r'although\\s+', r'despite\\s+',  # Contrasts\n","            r'because\\s+', r'since\\s+', r'due\\s+to',  # Explanations\n","            r'first\\s+', r'then\\s+', r'finally\\s+',  # Sequences\n","        ]\n","        complexity_score = sum(1 for pattern in complex_patterns if re.search(pattern, text))\n","        complexity_score = complexity_score / max(length, 1) if length > 0 else 0\n","\n","        # 5. Platform style classification\n","        if length < 10:\n","            platform_style = 'simple'\n","        elif detail_level > 0.1 and complexity_score > 0.05:\n","            platform_style = 'detailed'\n","        elif sentiment_clarity > 0.15:\n","            platform_style = 'emotional'\n","        else:\n","            platform_style = 'standard'\n","\n","        return {\n","            'length': length,\n","            'sentiment_clarity': sentiment_clarity,\n","            'detail_level': detail_level,\n","            'complexity_score': complexity_score,\n","            'platform_style': platform_style\n","        }\n","\n","    def intelligent_routing(self, review_characteristics):\n","        \"\"\"\n","        Intelligent routing based on review characteristics\n","        Returns weights for [transformer, bilstm, logistic]\n","        \"\"\"\n","        length = review_characteristics['length']\n","        sentiment_clarity = review_characteristics['sentiment_clarity']\n","        detail_level = review_characteristics['detail_level']\n","        complexity_score = review_characteristics['complexity_score']\n","        platform_style = review_characteristics['platform_style']\n","\n","        # Base weights\n","        weights = [0.33, 0.33, 0.34]  # [transformer, bilstm, logistic]\n","\n","        # Routing Logic for Cross-Domain Transfer (Yelp → Google)\n","\n","        # 1. Short Google reviews (< 15 words) - favor simpler models\n","        if length < 15:\n","            weights = [0.25, 0.45, 0.30]  # BiLSTM + Logistic for simple patterns\n","            primary_model = 'logistic'\n","\n","        # 2. Very detailed reviews (> 50 words) - favor Transformer\n","        elif length > 50:\n","            weights = [0.60, 0.25, 0.15]  # Transformer excels at long sequences\n","            primary_model = 'transformer'\n","\n","        # 3. High complexity or detailed mentions - Transformer\n","        elif complexity_score > 0.05 or detail_level > 0.1:\n","            weights = [0.55, 0.30, 0.15]  # Transformer for complex reasoning\n","            primary_model = 'transformer'\n","\n","        # 4. Clear emotional sentiment - balanced approach\n","        elif sentiment_clarity > 0.15:\n","            weights = [0.40, 0.40, 0.20]  # Transformer + BiLSTM for sentiment\n","            primary_model = 'logistic'\n","\n","        # 5. Medium length, standard reviews - slight Transformer preference\n","        elif 15 <= length <= 30:\n","            weights = [0.45, 0.35, 0.20]  # Standard routing\n","            primary_model = 'bilstm'\n","\n","        # 6. Ambiguous or neutral - favor ensemble diversity\n","        else:\n","            weights = [0.40, 0.35, 0.25]  # More balanced for uncertain cases\n","            primary_model = 'transformer'\n","\n","        # Platform-specific adjustments for Google reviews\n","        if platform_style == 'simple':\n","            # Google users often write shorter, simpler reviews\n","            weights[1] += 0.1  # Boost BiLSTM\n","            weights[0] -= 0.05  # Reduce Transformer\n","            weights[2] -= 0.05  # Reduce Logistic\n","\n","        elif platform_style == 'detailed':\n","            # Detailed Google reviews similar to Yelp\n","            weights[0] += 0.1  # Boost Transformer\n","            weights[1] -= 0.05  # Reduce others\n","            weights[2] -= 0.05\n","\n","        # Ensure weights sum to 1\n","        weights = np.array(weights)\n","        weights = weights / weights.sum()\n","\n","        return weights.tolist(), primary_model\n","\n","    def load_all_models(self):\n","        \"\"\"Load all three trained models with proper error handling\"\"\"\n","        print(\"🔄 Loading all Yelp-trained models...\")\n","\n","        # 1. Load Transformer\n","        try:\n","            transformer_checkpoint = torch.load('model/3class_transformer_v2.pth',\n","                                               map_location=self.device, weights_only=False)\n","            self.models['transformer'] = {\n","                'model': self.load_transformer_model(transformer_checkpoint),\n","                'vocab': transformer_checkpoint['vocab'],\n","                'config': transformer_checkpoint['model_config']\n","            }\n","            print(\"  ✅ Transformer loaded\")\n","        except Exception as e:\n","            print(f\"  ❌ Transformer loading failed: {e}\")\n","            print(\"  ⚠️  Skipping Transformer - will use BiLSTM + Logistic\")\n","\n","        # 2. Load BiLSTM\n","        try:\n","            bilstm_checkpoint = torch.load('model/3class_bilstm_yelp.pth',\n","                                         map_location=self.device, weights_only=False)\n","            self.models['bilstm'] = {\n","                'model': self.load_bilstm_model(bilstm_checkpoint),\n","                'vocab': bilstm_checkpoint['vocab'],\n","                'config': bilstm_checkpoint['config']\n","            }\n","            print(\"  ✅ BiLSTM loaded\")\n","        except Exception as e:\n","            print(f\"  ❌ BiLSTM loading failed: {e}\")\n","            print(\"  ⚠️  Skipping BiLSTM\")\n","\n","        # 3. Load Logistic\n","        try:\n","            self.models['logistic'] = {\n","                'model': joblib.load('model/3class_logistic_model_v2.pkl'),\n","                'vectorizer': joblib.load('model/3class_logistic_fidf_vectorizer_v2.pkl')\n","            }\n","            print(\"  ✅ Logistic loaded\")\n","        except Exception as e:\n","            print(f\"  ❌ Logistic loading failed: {e}\")\n","            print(\"  ⚠️  Skipping Logistic\")\n","\n","        print(f\"✅ Loaded {len(self.models)} models successfully!\")\n","\n","        # Adjust routing if some models failed to load\n","        if len(self.models) == 0:\n","            raise RuntimeError(\"❌ No models loaded successfully! Check model files.\")\n","        elif len(self.models) < 3:\n","            print(f\"⚠️  Only {len(self.models)} models loaded. Adjusting ensemble strategy...\")\n","            self.adjust_routing_for_available_models()\n","\n","    def load_transformer_model(self, checkpoint):\n","        \"\"\"Load transformer model architecture - COMPLETE IMPLEMENTATION\"\"\"\n","\n","        # Complete transformer architecture matching your training code\n","        class ImprovedMultiHeadAttention(nn.Module):\n","            def __init__(self, d_model, n_heads, dropout=0.1):\n","                super().__init__()\n","                self.d_model = d_model\n","                self.n_heads = n_heads\n","                self.d_k = d_model // n_heads\n","\n","                self.w_q = nn.Linear(d_model, d_model)\n","                self.w_k = nn.Linear(d_model, d_model)\n","                self.w_v = nn.Linear(d_model, d_model)\n","                self.w_o = nn.Linear(d_model, d_model)\n","\n","                self.dropout = nn.Dropout(dropout)\n","                self.layer_norm = nn.LayerNorm(d_model)\n","\n","            def forward(self, x, mask=None):\n","                batch_size, seq_len, d_model = x.size()\n","                residual = x\n","                x = self.layer_norm(x)\n","\n","                Q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n","                K = self.w_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n","                V = self.w_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n","\n","                attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n","\n","                if mask is not None:\n","                    attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n","\n","                attention_weights = torch.softmax(attention_scores, dim=-1)\n","                attention_weights = self.dropout(attention_weights)\n","\n","                context = torch.matmul(attention_weights, V)\n","                context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n","                output = self.w_o(context)\n","\n","                return residual + self.dropout(output)\n","\n","        class ImprovedTransformerBlock(nn.Module):\n","            def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n","                super().__init__()\n","                self.attention = ImprovedMultiHeadAttention(d_model, n_heads, dropout)\n","                self.feed_forward = nn.Sequential(\n","                    nn.LayerNorm(d_model),\n","                    nn.Linear(d_model, d_ff),\n","                    nn.GELU(),\n","                    nn.Dropout(dropout),\n","                    nn.Linear(d_ff, d_model)\n","                )\n","                self.dropout = nn.Dropout(dropout)\n","\n","            def forward(self, x, mask=None):\n","                x = self.attention(x, mask)\n","                residual = x\n","                ff_output = self.feed_forward(x)\n","                x = residual + self.dropout(ff_output)\n","                return x\n","\n","        class ImprovedTransformer(nn.Module):\n","            def __init__(self, vocab_size, d_model=128, n_heads=8, n_layers=4, d_ff=512, max_length=384,\n","                         num_classes=3, dropout=0.15):\n","                super().__init__()\n","                self.d_model = d_model\n","                self.max_length = max_length\n","\n","                self.token_embedding = nn.Embedding(vocab_size, d_model)\n","                self.position_embedding = nn.Embedding(max_length, d_model)\n","                self.embedding_dropout = nn.Dropout(dropout)\n","                self.embedding_norm = nn.LayerNorm(d_model)\n","\n","                self.transformer_blocks = nn.ModuleList([\n","                    ImprovedTransformerBlock(d_model, n_heads, d_ff, dropout)\n","                    for _ in range(n_layers)\n","                ])\n","\n","                self.final_norm = nn.LayerNorm(d_model)\n","                self.classifier = nn.Sequential(\n","                    nn.Linear(d_model * 2, d_model),  # *2 for concatenated pooling\n","                    nn.GELU(),\n","                    nn.Dropout(dropout),\n","                    nn.Linear(d_model, d_model // 2),\n","                    nn.GELU(),\n","                    nn.Dropout(dropout),\n","                    nn.Linear(d_model // 2, num_classes)\n","                )\n","\n","                self.init_weights()\n","\n","            def init_weights(self):\n","                for module in self.modules():\n","                    if isinstance(module, nn.Linear):\n","                        torch.nn.init.xavier_uniform_(module.weight)\n","                        if module.bias is not None:\n","                            torch.nn.init.zeros_(module.bias)\n","                    elif isinstance(module, nn.Embedding):\n","                        torch.nn.init.normal_(module.weight, mean=0, std=0.02)\n","\n","            def forward(self, x):\n","                batch_size, seq_len = x.size()\n","                positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n","\n","                token_emb = self.token_embedding(x) * math.sqrt(self.d_model)\n","                pos_emb = self.position_embedding(positions)\n","                embeddings = token_emb + pos_emb\n","                embeddings = self.embedding_norm(embeddings)\n","                embeddings = self.embedding_dropout(embeddings)\n","\n","                pad_mask = (x != 0).unsqueeze(1).unsqueeze(1)\n","\n","                x = embeddings\n","                for transformer in self.transformer_blocks:\n","                    x = transformer(x, pad_mask)\n","\n","                x = self.final_norm(x)\n","\n","                # Dual pooling\n","                mask = (x.sum(dim=-1) != 0).float().unsqueeze(-1)\n","                x_mean = (x * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-9)\n","                x_max, _ = (x + (1 - mask) * (-1e9)).max(dim=1)\n","                x_pooled = torch.cat([x_mean, x_max], dim=-1)\n","\n","                logits = self.classifier(x_pooled)\n","                return logits\n","\n","        model = ImprovedTransformer(**checkpoint['model_config'])\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        model.to(self.device)\n","        model.eval()\n","        return model\n","\n","    def load_bilstm_model(self, checkpoint):\n","        \"\"\"Load BiLSTM model architecture\"\"\"\n","        class BiLSTMClassifier(nn.Module):\n","            def __init__(self, vocab_size, emb_dim, hidden_dim, n_classes, padding_idx, n_layers=1, dropout=0.5):\n","                super().__init__()\n","                self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=padding_idx)\n","                self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=n_layers,\n","                                    bidirectional=True, batch_first=True,\n","                                    dropout=dropout if n_layers>1 else 0)\n","                self.dropout = nn.Dropout(dropout)\n","                self.fc = nn.Linear(hidden_dim*2, n_classes)\n","\n","            def forward(self, x):\n","                x_emb = self.embedding(x)\n","                _, (h_n, _) = self.lstm(x_emb)\n","                h_f = h_n[-2]  # forward final\n","                h_b = h_n[-1]  # backward final\n","                h   = torch.cat([h_f, h_b], dim=1)\n","                return self.fc(self.dropout(h))\n","\n","        config = checkpoint['config']\n","        vocab = checkpoint['vocab']\n","\n","        model = BiLSTMClassifier(\n","            vocab_size=len(vocab['itos']),\n","            emb_dim=config['embed_dim'],\n","            hidden_dim=config['hidden_dim'],\n","            n_classes=config['n_classes'],\n","            padding_idx=vocab['stoi']['<PAD>']\n","        )\n","\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        model.to(self.device)\n","        model.eval()\n","        return model\n","\n","    def predict_single_model(self, texts, model_name):\n","        \"\"\"Get predictions from a single model\"\"\"\n","        if model_name not in self.models:\n","            raise ValueError(f\"Model {model_name} not loaded\")\n","\n","        if model_name == 'transformer':\n","            return self.predict_transformer(texts)\n","        elif model_name == 'bilstm':\n","            return self.predict_bilstm(texts)\n","        elif model_name == 'logistic':\n","            return self.predict_logistic(texts)\n","\n","    def adjust_routing_for_available_models(self):\n","        \"\"\"Adjust routing strategy based on available models\"\"\"\n","        available_models = list(self.models.keys())\n","        print(f\"🔧 Adjusting routing for available models: {available_models}\")\n","\n","        # Update routing stats to only track available models\n","        self.routing_stats = {model: 0 for model in available_models}\n","\n","        # Store original routing function\n","        self.original_intelligent_routing = self.intelligent_routing\n","\n","        # Create new routing function for available models\n","        def adjusted_routing(characteristics):\n","            if len(available_models) == 1:\n","                # Only one model available\n","                model = available_models[0]\n","                return [1.0], model\n","\n","            elif len(available_models) == 2:\n","                # Two models available\n","                if 'transformer' in available_models and 'bilstm' in available_models:\n","                    # Transformer + BiLSTM\n","                    if characteristics['length'] > 30 or characteristics['complexity_score'] > 0.05:\n","                        return [0.7, 0.3], 'transformer'\n","                    else:\n","                        return [0.4, 0.6], 'bilstm'\n","\n","                elif 'transformer' in available_models and 'logistic' in available_models:\n","                    # Transformer + Logistic\n","                    if characteristics['length'] > 20:\n","                        return [0.8, 0.2], 'transformer'\n","                    else:\n","                        return [0.5, 0.5], 'transformer'\n","\n","                elif 'bilstm' in available_models and 'logistic' in available_models:\n","                    # BiLSTM + Logistic\n","                    if characteristics['length'] < 15:\n","                        return [0.3, 0.7], 'logistic'\n","                    else:\n","                        return [0.7, 0.3], 'bilstm'\n","\n","            # Fallback to original if all three available\n","            return self.original_intelligent_routing(characteristics)\n","\n","        # Replace routing function\n","        self.intelligent_routing = adjusted_routing\n","\n","    def predict_transformer(self, texts):\n","        \"\"\"Predict with transformer model using your exact preprocessing\"\"\"\n","        model = self.models['transformer']['model']\n","        vocab = self.models['transformer']['vocab']\n","\n","        # Use the exact preprocessing from your transformer inference code\n","        class ImprovedYelpDataset:\n","            def __init__(self, max_length=384):\n","                self.max_length = max_length\n","\n","            def enhanced_tokenize(self, text):\n","                if pd.isna(text) or text == \"\":\n","                    return [\"<UNK>\"]\n","\n","                text = str(text).lower()\n","\n","                # Handle contractions\n","                text = re.sub(r\"won't\", \"will not\", text)\n","                text = re.sub(r\"can't\", \"cannot\", text)\n","                text = re.sub(r\"n't\", \" not\", text)\n","                text = re.sub(r\"'re\", \" are\", text)\n","                text = re.sub(r\"'ve\", \" have\", text)\n","                text = re.sub(r\"'ll\", \" will\", text)\n","                text = re.sub(r\"'d\", \" would\", text)\n","                text = re.sub(r\"'m\", \" am\", text)\n","\n","                # Handle sentiment patterns\n","                text = re.sub(r'[!]{2,}', ' very_excited ', text)\n","                text = re.sub(r'[?]{2,}', ' very_confused ', text)\n","                text = re.sub(r'[.]{3,}', ' continuation ', text)\n","\n","                # Clean punctuation\n","                text = re.sub(r'[^a-zA-Z0-9\\s!?.]', ' ', text)\n","                text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n","                text = re.sub(r'\\s+', ' ', text).strip()\n","\n","                return text.split()\n","\n","            def preprocess_text(self, text, vocab):\n","                tokens = self.enhanced_tokenize(text)\n","                token_ids = [vocab.get(token, vocab.get('<UNK>', 1)) for token in tokens]\n","\n","                if len(token_ids) > self.max_length:\n","                    token_ids = token_ids[:self.max_length]\n","                else:\n","                    token_ids += [vocab.get('<PAD>', 0)] * (self.max_length - len(token_ids))\n","\n","                return torch.tensor(token_ids, dtype=torch.long)\n","\n","        dataset_processor = ImprovedYelpDataset(max_length=384)\n","        predictions = []\n","        probabilities = []\n","\n","        # Process in batches\n","        batch_size = 32\n","        for i in range(0, len(texts), batch_size):\n","            batch_texts = texts[i:i+batch_size]\n","\n","            batch_tensors = []\n","            for text in batch_texts:\n","                tensor = dataset_processor.preprocess_text(text, vocab)\n","                batch_tensors.append(tensor)\n","\n","            batch_input = torch.stack(batch_tensors).to(self.device)\n","\n","            with torch.no_grad():\n","                outputs = model(batch_input)\n","                probs = torch.softmax(outputs, dim=1)\n","                preds = outputs.argmax(dim=1)\n","\n","                predictions.extend(preds.cpu().numpy())\n","                probabilities.extend(probs.cpu().numpy())\n","\n","        return np.array(predictions), np.array(probabilities)\n","\n","    def predict_bilstm(self, texts):\n","        \"\"\"Predict with BiLSTM model using your exact preprocessing\"\"\"\n","        model = self.models['bilstm']['model']\n","        vocab = self.models['bilstm']['vocab']\n","        config = self.models['bilstm']['config']\n","\n","        # Use exact preprocessing from your BiLSTM code\n","        def preprocess_text(text):\n","            text = str(text).lower()\n","            text = re.sub(r'[^a-z0-9\\s]', '', text)\n","            return text.split()\n","\n","        predictions = []\n","        probabilities = []\n","\n","        # Process in batches\n","        batch_size = 128\n","        for i in range(0, len(texts), batch_size):\n","            batch_texts = texts[i:i+batch_size]\n","\n","            batch_tensors = []\n","            for text in batch_texts:\n","                tokens = preprocess_text(text)\n","                token_ids = [vocab['stoi'].get(token, vocab['stoi']['<UNK>']) for token in tokens]\n","\n","                max_len = config.get('max_len', 200)\n","                if len(token_ids) < max_len:\n","                    token_ids += [vocab['stoi']['<PAD>']] * (max_len - len(token_ids))\n","                else:\n","                    token_ids = token_ids[:max_len]\n","\n","                batch_tensors.append(torch.tensor(token_ids, dtype=torch.long))\n","\n","            batch_input = torch.stack(batch_tensors).to(self.device)\n","\n","            with torch.no_grad():\n","                outputs = model(batch_input)\n","                probs = torch.softmax(outputs, dim=1)\n","                preds = outputs.argmax(dim=1)\n","\n","                predictions.extend(preds.cpu().numpy())\n","                probabilities.extend(probs.cpu().numpy())\n","\n","        return np.array(predictions), np.array(probabilities)\n","\n","    def predict_logistic(self, texts):\n","        \"\"\"Predict with Logistic model\"\"\"\n","        model = self.models['logistic']['model']\n","        vectorizer = self.models['logistic']['vectorizer']\n","\n","        # Vectorize texts\n","        X_tfidf = vectorizer.transform(texts)\n","\n","        # Predict\n","        predictions = model.predict(X_tfidf)\n","        probabilities = model.predict_proba(X_tfidf)\n","\n","        return predictions, probabilities\n","\n","    def adaptive_ensemble_predict(self, texts, return_routing_info=False):\n","        \"\"\"\n","        Main adaptive ensemble prediction function with fallback handling\n","        \"\"\"\n","        print(f\"🎯 Running Adaptive MoE on {len(texts)} reviews...\")\n","\n","        available_models = list(self.models.keys())\n","        print(f\"📋 Available models: {available_models}\")\n","\n","        if len(available_models) == 0:\n","            raise RuntimeError(\"No models available for prediction!\")\n","\n","        ensemble_predictions = []\n","        ensemble_probabilities = []\n","        routing_decisions = []\n","\n","        # Get predictions from available models only\n","        print(\"  Getting predictions from available models...\")\n","        model_predictions = {}\n","        model_probabilities = {}\n","\n","        for model_name in available_models:\n","            try:\n","                preds, probs = self.predict_single_model(texts, model_name)\n","                model_predictions[model_name] = preds\n","                model_probabilities[model_name] = probs\n","                print(f\"    ✅ {model_name} predictions complete\")\n","            except Exception as e:\n","                print(f\"    ❌ {model_name} prediction failed: {e}\")\n","                # Remove failed model from available models\n","                if model_name in available_models:\n","                    available_models.remove(model_name)\n","\n","        if len(available_models) == 0:\n","            raise RuntimeError(\"All model predictions failed!\")\n","\n","        print(\"  Applying adaptive routing...\")\n","\n","        for i, text in enumerate(texts):\n","            # Analyze review characteristics\n","            characteristics = self.analyze_review_characteristics(text)\n","\n","            # Get routing weights for available models\n","            weights, primary_model = self.intelligent_routing(characteristics)\n","\n","            # Ensure primary model is available\n","            if primary_model not in available_models:\n","                primary_model = available_models[0]\n","\n","            # Track routing statistics\n","            self.routing_stats[primary_model] += 1\n","\n","            # Handle different numbers of available models\n","            if len(available_models) == 1:\n","                # Only one model available\n","                model_name = available_models[0]\n","                ensemble_prob = model_probabilities[model_name][i]\n","                weights = [1.0]\n","\n","            elif len(available_models) == 2:\n","                # Two models available\n","                model1, model2 = available_models\n","                ensemble_prob = (weights[0] * model_probabilities[model1][i] +\n","                               weights[1] * model_probabilities[model2][i])\n","\n","            else:\n","                # All three models available\n","                ensemble_prob = (weights[0] * model_probabilities['transformer'][i] +\n","                               weights[1] * model_probabilities['bilstm'][i] +\n","                               weights[2] * model_probabilities['logistic'][i])\n","\n","            ensemble_pred = np.argmax(ensemble_prob)\n","\n","            ensemble_predictions.append(ensemble_pred)\n","            ensemble_probabilities.append(ensemble_prob)\n","\n","            if return_routing_info:\n","                routing_decisions.append({\n","                    'weights': weights,\n","                    'primary_model': primary_model,\n","                    'characteristics': characteristics,\n","                    'available_models': available_models.copy()\n","                })\n","\n","        if return_routing_info:\n","            return (np.array(ensemble_predictions),\n","                   np.array(ensemble_probabilities),\n","                   routing_decisions)\n","\n","        return np.array(ensemble_predictions), np.array(ensemble_probabilities)\n","\n","    def evaluate_on_google_reviews(self, google_reviews_df, review_column='review', star_column='stars'):\n","        \"\"\"\n","        Evaluate adaptive ensemble on Google reviews dataset\n","        \"\"\"\n","        print(\"🌐 EVALUATING ADAPTIVE MOE ON GOOGLE REVIEWS\")\n","        print(\"=\"*60)\n","\n","        # Data preprocessing\n","        print(\"Preprocessing Google reviews...\")\n","\n","        # Remove missing reviews\n","        initial_count = len(google_reviews_df)\n","        google_reviews_df = google_reviews_df.dropna(subset=[review_column, star_column])\n","        google_reviews_df = google_reviews_df[google_reviews_df[review_column].str.strip() != '']\n","\n","        print(f\"✓ Cleaned data: {len(google_reviews_df)} reviews ({initial_count - len(google_reviews_df)} removed)\")\n","\n","        # Convert to 3-class labels (same as Yelp training)\n","        def convert_stars_to_3class(stars):\n","            if stars <= 2:\n","                return 0  # Negative\n","            elif stars == 3:\n","                return 1  # Neutral\n","            else:  # stars >= 4\n","                return 2  # Positive\n","\n","        google_reviews_df['true_label'] = google_reviews_df[star_column].apply(convert_stars_to_3class)\n","\n","        # Print distribution\n","        print(\"\\nGoogle Reviews Distribution:\")\n","        class_dist = google_reviews_df['true_label'].value_counts().sort_index()\n","        class_names = ['Negative (≤2★)', 'Neutral (3★)', 'Positive (≥4★)']\n","        for i, count in enumerate(class_dist):\n","            print(f\"  {class_names[i]}: {count} ({count/len(google_reviews_df)*100:.1f}%)\")\n","\n","        # Make predictions\n","        texts = google_reviews_df[review_column].tolist()\n","        true_labels = google_reviews_df['true_label'].tolist()\n","\n","        start_time = time.time()\n","        ensemble_preds, ensemble_probs, routing_info = self.adaptive_ensemble_predict(\n","            texts, return_routing_info=True\n","        )\n","        inference_time = time.time() - start_time\n","\n","        print(f\"✓ Inference completed in {inference_time:.2f} seconds\")\n","        print(f\"✓ Average time per review: {inference_time/len(texts)*1000:.2f}ms\")\n","\n","        # Routing statistics\n","        print(f\"\\n📊 ROUTING STATISTICS:\")\n","        total_routes = sum(self.routing_stats.values())\n","        for model, count in self.routing_stats.items():\n","            percentage = count / total_routes * 100 if total_routes > 0 else 0\n","            print(f\"  {model.capitalize():>11}: {count:>5} routes ({percentage:>5.1f}%)\")\n","\n","        # Calculate metrics\n","        accuracy = accuracy_score(true_labels, ensemble_preds)\n","        macro_f1 = f1_score(true_labels, ensemble_preds, average='macro')\n","        weighted_f1 = f1_score(true_labels, ensemble_preds, average='weighted')\n","\n","        # Print results\n","        print(f\"\\n🎯 ADAPTIVE MOE PERFORMANCE:\")\n","        print(\"=\"*40)\n","        print(f\"Cross-Domain Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n","        print(f\"Macro F1-Score: {macro_f1:.4f}\")\n","        print(f\"Weighted F1-Score: {weighted_f1:.4f}\")\n","\n","        # Detailed classification report\n","        print(f\"\\n📊 DETAILED RESULTS:\")\n","        target_names = ['Negative', 'Neutral', 'Positive']\n","        report = classification_report(true_labels, ensemble_preds, target_names=target_names, digits=4)\n","        print(report)\n","\n","        # Compare with individual models\n","        print(f\"\\n🔍 INDIVIDUAL MODEL COMPARISON:\")\n","        print(\"-\"*50)\n","\n","        # Get individual model predictions for comparison\n","        transformer_preds, _ = self.predict_single_model(texts, 'transformer')\n","        bilstm_preds, _ = self.predict_single_model(texts, 'bilstm')\n","        logistic_preds, _ = self.predict_single_model(texts, 'logistic')\n","\n","        models_performance = {\n","            'Transformer': accuracy_score(true_labels, transformer_preds),\n","            'BiLSTM': accuracy_score(true_labels, bilstm_preds),\n","            'Logistic': accuracy_score(true_labels, logistic_preds),\n","            'Adaptive MoE': accuracy\n","        }\n","\n","        for model_name, acc in models_performance.items():\n","            print(f\"{model_name:>13}: {acc:.4f} accuracy\")\n","\n","        best_individual = max(models_performance.items(), key=lambda x: x[1] if x[0] != 'Adaptive MoE' else 0)\n","        improvement = accuracy - best_individual[1]\n","\n","        print(f\"\\n🏆 ENSEMBLE IMPROVEMENT:\")\n","        print(f\"Best Individual: {best_individual[0]} ({best_individual[1]:.4f})\")\n","        print(f\"Adaptive MoE:    {accuracy:.4f}\")\n","        print(f\"Improvement:     +{improvement:.4f} ({improvement*100:+.2f}%)\")\n","\n","        # Save results\n","        google_reviews_df['review_index'] = google_reviews_df['review_index']\n","        google_reviews_df['ensemble_prediction'] = ensemble_preds\n","        google_reviews_df['ensemble_confidence'] = ensemble_probs.max(axis=1)\n","        google_reviews_df['prob_negative'] = ensemble_probs[:, 0]\n","        google_reviews_df['prob_neutral'] = ensemble_probs[:, 1]\n","        google_reviews_df['prob_positive'] = ensemble_probs[:, 2]\n","\n","        # Add routing information\n","        for i, route_info in enumerate(routing_info):\n","            google_reviews_df.loc[i, 'primary_model'] = route_info['primary_model']\n","            google_reviews_df.loc[i, 'transformer_weight'] = route_info['weights'][0]\n","            google_reviews_df.loc[i, 'bilstm_weight'] = route_info['weights'][1]\n","            google_reviews_df.loc[i, 'logistic_weight'] = route_info['weights'][2]\n","\n","        output_file = 'google_reviews_adaptive_moe_results.csv'\n","        google_reviews_df.to_csv(output_file, index=False)\n","        print(f\"\\n💾 Results saved to '{output_file}'\")\n","\n","        return google_reviews_df, accuracy, macro_f1\n","\n","# Example usage function\n","def run_adaptive_moe_on_google_reviews():\n","    \"\"\"\n","    Main function to run Adaptive MoE on Google reviews\n","    \"\"\"\n","    print(\"🚀 ADAPTIVE MIXTURE OF EXPERTS: YELP → GOOGLE\")\n","    print(\"=\"*60)\n","\n","    # Initialize Adaptive MoE\n","    moe = AdaptiveMixtureOfExperts()\n","\n","    # Load all models\n","    moe.load_all_models()\n","\n","    # Load Google reviews (replace with your actual Google reviews dataset)\n","    print(\"📱 Loading Google reviews...\")\n","    try:\n","        # Replace this with your actual Google reviews CSV\n","        google_reviews = pd.read_csv(\"datastore/Google_Reviews.csv\")\n","        #google_reviews  = pd.read_parquet(\"datastore/test-00000-of-00001.parquet\")\n","        print(f\"✓ Loaded {len(google_reviews)} Google reviews\")\n","    except FileNotFoundError:\n","        print(\"⚠️  Using USS reviews as proxy for Google reviews...\")\n","        # Use USS reviews as a proxy for Google reviews\n","        google_reviews = pd.read_csv(\"datastore/USS_Reviews_Silver.csv\", parse_dates=[\"publishedAtDate\"])\n","        # Rename columns to match expected format\n","        google_reviews = google_reviews.rename(columns={'review': 'review', 'stars': 'stars'})\n","\n","\n","    # Run evaluation\n","    results_df, accuracy, macro_f1 = moe.evaluate_on_google_reviews(\n","        google_reviews,\n","        review_column='review',\n","        star_column='stars'\n","    )\n","\n","    print(f\"\\n🎉 ADAPTIVE MOE COMPLETE!\")\n","    print(f\"Final Cross-Domain Accuracy: {accuracy:.4f}\")\n","    print(f\"Expected improvement over single models: +2-5%\")\n","\n","    return results_df, moe\n","\n","if __name__ == \"__main__\":\n","    # Run the adaptive ensemble\n","    results, moe_system = run_adaptive_moe_on_google_reviews()"],"metadata":{"id":"zsQqmKxfCKqo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750353019526,"user_tz":-480,"elapsed":48320,"user":{"displayName":"Kindness Matters","userId":"04525017512802736418"}},"outputId":"02012eb5-fe24-40d4-939c-d8dea5b7ff12"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["🚀 ADAPTIVE MIXTURE OF EXPERTS: YELP → GOOGLE\n","============================================================\n","🧠 Adaptive MoE initialized on cuda\n","🔄 Loading all Yelp-trained models...\n","  ✅ Transformer loaded\n","  ✅ BiLSTM loaded\n","  ✅ Logistic loaded\n","✅ Loaded 3 models successfully!\n","📱 Loading Google reviews...\n","⚠️  Using USS reviews as proxy for Google reviews...\n","🌐 EVALUATING ADAPTIVE MOE ON GOOGLE REVIEWS\n","============================================================\n","Preprocessing Google reviews...\n","✓ Cleaned data: 29412 reviews (0 removed)\n","\n","Google Reviews Distribution:\n","  Negative (≤2★): 2206 (7.5%)\n","  Neutral (3★): 2133 (7.3%)\n","  Positive (≥4★): 25073 (85.2%)\n","🎯 Running Adaptive MoE on 29412 reviews...\n","📋 Available models: ['transformer', 'bilstm', 'logistic']\n","  Getting predictions from available models...\n","    ✅ transformer predictions complete\n","    ✅ bilstm predictions complete\n","    ✅ logistic predictions complete\n","  Applying adaptive routing...\n","✓ Inference completed in 15.22 seconds\n","✓ Average time per review: 0.52ms\n","\n","📊 ROUTING STATISTICS:\n","  Transformer:  9058 routes ( 30.8%)\n","       Bilstm:  5098 routes ( 17.3%)\n","     Logistic: 15256 routes ( 51.9%)\n","\n","🎯 ADAPTIVE MOE PERFORMANCE:\n","========================================\n","Cross-Domain Accuracy: 0.6882 (68.82%)\n","Macro F1-Score: 0.5184\n","Weighted F1-Score: 0.7488\n","\n","📊 DETAILED RESULTS:\n","              precision    recall  f1-score   support\n","\n","    Negative     0.3864    0.8214    0.5255      2206\n","     Neutral     0.1414    0.4576    0.2161      2133\n","    Positive     0.9793    0.6960    0.8137     25073\n","\n","    accuracy                         0.6882     29412\n","   macro avg     0.5024    0.6583    0.5184     29412\n","weighted avg     0.8741    0.6882    0.7488     29412\n","\n","\n","🔍 INDIVIDUAL MODEL COMPARISON:\n","--------------------------------------------------\n","  Transformer: 0.6868 accuracy\n","       BiLSTM: 0.6815 accuracy\n","     Logistic: 0.5930 accuracy\n"," Adaptive MoE: 0.6882 accuracy\n","\n","🏆 ENSEMBLE IMPROVEMENT:\n","Best Individual: Transformer (0.6868)\n","Adaptive MoE:    0.6882\n","Improvement:     +0.0014 (+0.14%)\n","\n","💾 Results saved to 'google_reviews_adaptive_moe_results.csv'\n","\n","🎉 ADAPTIVE MOE COMPLETE!\n","Final Cross-Domain Accuracy: 0.6882\n","Expected improvement over single models: +2-5%\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMYmNo3FlA+b2yFMtE3le6F"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}