{"cells":[{"cell_type":"markdown","source":["# 3-Class Transformer Inference & Evaluation on USS Reviews\n","\n","**High-level summary:**  \n","Loads a saved Transformer checkpoint, preprocesses USS review text with an enhanced tokenizer, runs batched inference to produce 3-class sentiment predictions and probability scores, performs detailed evaluation (accuracy, F1, confusion matrix, per-class stats), saves results and examples to CSV, and provides a single-review prediction utility."],"metadata":{"id":"kVgQx_84Gaax"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3536,"status":"ok","timestamp":1750338447942,"user":{"displayName":"Kindness Matters","userId":"04525017512802736418"},"user_tz":-480},"id":"LjqAXWWnCZr9","outputId":"dc7c6b73-84a5-445e-eb34-9dee76aaaa5e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# prompt: connect google drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# prompt: load current directory\n","\n","import os\n","\n","os.chdir('/content/drive/My Drive/CS605-NLP-Project')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mUAMIK1D_Ys"},"outputs":[],"source":["# Install\n","#!pip install --upgrade numpy gensim --no-cache-dir\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IDvyCWXQDXg0"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n","import re\n","import math\n","import time\n","from collections import Counter\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","class ImprovedYelpDataset:\n","    \"\"\"Dataset class for inference (no torch Dataset needed)\"\"\"\n","    def __init__(self, max_length=384):\n","        self.max_length = max_length\n","\n","    def enhanced_tokenize(self, text):\n","        \"\"\"Enhanced tokenization with better preprocessing\"\"\"\n","        if pd.isna(text) or text == \"\":\n","            return [\"<UNK>\"]\n","\n","        # Convert to string and lowercase\n","        text = str(text).lower()\n","\n","        # Handle contractions and common patterns\n","        text = re.sub(r\"won't\", \"will not\", text)\n","        text = re.sub(r\"can't\", \"cannot\", text)\n","        text = re.sub(r\"n't\", \" not\", text)\n","        text = re.sub(r\"'re\", \" are\", text)\n","        text = re.sub(r\"'ve\", \" have\", text)\n","        text = re.sub(r\"'ll\", \" will\", text)\n","        text = re.sub(r\"'d\", \" would\", text)\n","        text = re.sub(r\"'m\", \" am\", text)\n","\n","        # Handle punctuation - keep some sentiment-relevant ones\n","        text = re.sub(r'[!]{2,}', ' very_excited ', text)  # Multiple exclamations\n","        text = re.sub(r'[?]{2,}', ' very_confused ', text)  # Multiple questions\n","        text = re.sub(r'[.]{3,}', ' continuation ', text)   # Ellipsis\n","\n","        # Remove remaining punctuation except basic ones\n","        text = re.sub(r'[^a-zA-Z0-9\\s!?.]', ' ', text)\n","\n","        # Handle repeated characters (e.g., \"soooo good\" -> \"so good\")\n","        text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n","\n","        # Clean whitespace\n","        text = re.sub(r'\\s+', ' ', text).strip()\n","\n","        return text.split()\n","\n","    def preprocess_text(self, text, vocab):\n","        \"\"\"Preprocess single text for inference\"\"\"\n","        tokens = self.enhanced_tokenize(text)\n","        token_ids = [vocab.get(token, vocab.get('<UNK>', 1)) for token in tokens]\n","\n","        # Pad or truncate\n","        if len(token_ids) > self.max_length:\n","            token_ids = token_ids[:self.max_length]\n","        else:\n","            token_ids += [vocab.get('<PAD>', 0)] * (self.max_length - len(token_ids))\n","\n","        return torch.tensor(token_ids, dtype=torch.long)\n","\n","# Model Architecture (same as training)\n","class ImprovedMultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, n_heads, dropout=0.1):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.n_heads = n_heads\n","        self.d_k = d_model // n_heads\n","\n","        self.w_q = nn.Linear(d_model, d_model)\n","        self.w_k = nn.Linear(d_model, d_model)\n","        self.w_v = nn.Linear(d_model, d_model)\n","        self.w_o = nn.Linear(d_model, d_model)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.layer_norm = nn.LayerNorm(d_model)\n","\n","    def forward(self, x, mask=None):\n","        batch_size, seq_len, d_model = x.size()\n","        residual = x\n","        x = self.layer_norm(x)\n","\n","        Q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n","        K = self.w_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n","        V = self.w_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n","\n","        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n","\n","        if mask is not None:\n","            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n","\n","        attention_weights = torch.softmax(attention_scores, dim=-1)\n","        attention_weights = self.dropout(attention_weights)\n","\n","        context = torch.matmul(attention_weights, V)\n","        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n","        output = self.w_o(context)\n","\n","        return residual + self.dropout(output)\n","\n","class ImprovedTransformerBlock(nn.Module):\n","    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n","        super().__init__()\n","        self.attention = ImprovedMultiHeadAttention(d_model, n_heads, dropout)\n","        self.feed_forward = nn.Sequential(\n","            nn.LayerNorm(d_model),\n","            nn.Linear(d_model, d_ff),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(d_ff, d_model)\n","        )\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask=None):\n","        x = self.attention(x, mask)\n","        residual = x\n","        ff_output = self.feed_forward(x)\n","        x = residual + self.dropout(ff_output)\n","        return x\n","\n","class ImprovedTransformer(nn.Module):\n","    def __init__(self, vocab_size, d_model=128, n_heads=8, n_layers=4, d_ff=512, max_length=384,\n","                 num_classes=3, dropout=0.15):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.max_length = max_length\n","\n","        self.token_embedding = nn.Embedding(vocab_size, d_model)\n","        self.position_embedding = nn.Embedding(max_length, d_model)\n","        self.embedding_dropout = nn.Dropout(dropout)\n","        self.embedding_norm = nn.LayerNorm(d_model)\n","\n","        self.transformer_blocks = nn.ModuleList([\n","            ImprovedTransformerBlock(d_model, n_heads, d_ff, dropout)\n","            for _ in range(n_layers)\n","        ])\n","\n","        self.final_norm = nn.LayerNorm(d_model)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(d_model * 2, d_model),  # *2 for concatenated pooling\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(d_model, d_model // 2),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(d_model // 2, num_classes)\n","        )\n","\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        for module in self.modules():\n","            if isinstance(module, nn.Linear):\n","                torch.nn.init.xavier_uniform_(module.weight)\n","                if module.bias is not None:\n","                    torch.nn.init.zeros_(module.bias)\n","            elif isinstance(module, nn.Embedding):\n","                torch.nn.init.normal_(module.weight, mean=0, std=0.02)\n","\n","    def forward(self, x):\n","        batch_size, seq_len = x.size()\n","        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n","\n","        token_emb = self.token_embedding(x) * math.sqrt(self.d_model)\n","        pos_emb = self.position_embedding(positions)\n","        embeddings = token_emb + pos_emb\n","        embeddings = self.embedding_norm(embeddings)\n","        embeddings = self.embedding_dropout(embeddings)\n","\n","        pad_mask = (x != 0).unsqueeze(1).unsqueeze(1)\n","\n","        x = embeddings\n","        for transformer in self.transformer_blocks:\n","            x = transformer(x, pad_mask)\n","\n","        x = self.final_norm(x)\n","\n","        # Dual pooling\n","        mask = (x.sum(dim=-1) != 0).float().unsqueeze(-1)\n","        x_mean = (x * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-9)\n","        x_max, _ = (x + (1 - mask) * (-1e9)).max(dim=1)\n","        x_pooled = torch.cat([x_mean, x_max], dim=-1)\n","\n","        logits = self.classifier(x_pooled)\n","        return logits\n","\n","def load_trained_model(model_path='model/3class_transformer_v2.pth'):\n","    \"\"\"Load the trained model and vocabulary\"\"\"\n","    print(f\"Loading model from {model_path}...\")\n","\n","    # Load checkpoint\n","    checkpoint = torch.load(model_path, map_location=device)\n","\n","    # Extract model config and vocab\n","    model_config = checkpoint['model_config']\n","    vocab = checkpoint['vocab']\n","\n","    # Initialize model with same architecture\n","    model = ImprovedTransformer(**model_config)\n","\n","    # Load state dict\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model.to(device)\n","    model.eval()\n","\n","    print(f\"‚úì Model loaded successfully!\")\n","    print(f\"‚úì Vocabulary size: {len(vocab)}\")\n","    print(f\"‚úì Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n","\n","    return model, vocab\n","\n","def convert_stars_to_3class(stars):\n","    \"\"\"Convert star ratings to 3-class labels\"\"\"\n","    if stars <= 2:\n","        return 0  # Negative\n","    elif stars == 3:\n","        return 1  # Neutral\n","    else:  # stars >= 3\n","        return 2  # Positive\n","\n","def predict_batch(model, texts, vocab, batch_size=64, max_length=384):\n","    \"\"\"Predict sentiment for a batch of texts\"\"\"\n","    model.eval()\n","    dataset_processor = ImprovedYelpDataset(max_length=max_length)\n","\n","    all_predictions = []\n","    all_probabilities = []\n","\n","    # Process in batches\n","    for i in range(0, len(texts), batch_size):\n","        batch_texts = texts[i:i+batch_size]\n","\n","        # Preprocess batch\n","        batch_tensors = []\n","        for text in batch_texts:\n","            tensor = dataset_processor.preprocess_text(text, vocab)\n","            batch_tensors.append(tensor)\n","\n","        # Stack into batch\n","        batch_input = torch.stack(batch_tensors).to(device)\n","\n","        # Predict\n","        with torch.no_grad():\n","            outputs = model(batch_input)\n","            probabilities = torch.softmax(outputs, dim=1)\n","            predictions = outputs.argmax(dim=1)\n","\n","            all_predictions.extend(predictions.cpu().numpy())\n","            all_probabilities.extend(probabilities.cpu().numpy())\n","\n","    return np.array(all_predictions), np.array(all_probabilities)\n","\n","def evaluate_on_uss_reviews(model_path='model/3class_transformer_v2.pth'):\n","    \"\"\"Main inference function for USS Reviews dataset\"\"\"\n","\n","    print(\"=\"*60)\n","    print(\"üé¨ USS REVIEWS SENTIMENT ANALYSIS\")\n","    print(\"=\"*60)\n","\n","    # Load the dataset\n","    print(\"Loading USS Reviews dataset...\")\n","    try:\n","        uss_reviews = pd.read_csv(\"datastore/USS_Reviews_Silver.csv\", parse_dates=[\"publishedAtDate\"])\n","        print(f\"‚úì Loaded {len(uss_reviews)} reviews\")\n","    except FileNotFoundError:\n","        print(\"‚ùå Error: Could not find 'datastore/USS_Reviews_Silver.csv'\")\n","        print(\"Please check the file path and try again.\")\n","        return\n","\n","    # Check required columns\n","    if 'review' not in uss_reviews.columns or 'stars' not in uss_reviews.columns:\n","        print(\"‚ùå Error: Required columns 'review' and 'stars' not found\")\n","        print(f\"Available columns: {list(uss_reviews.columns)}\")\n","        return\n","\n","    # Data cleaning\n","    print(\"Preprocessing data...\")\n","\n","    # Remove rows with missing reviews or stars\n","    initial_count = len(uss_reviews)\n","    uss_reviews = uss_reviews.dropna(subset=['review', 'stars'])\n","    uss_reviews = uss_reviews[uss_reviews['review'].str.strip() != '']\n","\n","    print(f\"‚úì Cleaned data: {len(uss_reviews)} reviews ({initial_count - len(uss_reviews)} removed)\")\n","\n","    # Convert stars to 3-class labels\n","    uss_reviews['true_label'] = uss_reviews['stars'].apply(convert_stars_to_3class)\n","\n","    # Print distribution\n","    print(\"\\nUSS Reviews Star Distribution:\")\n","    star_dist = uss_reviews['stars'].value_counts().sort_index()\n","    print(star_dist)\n","\n","    print(\"\\nUSS Reviews 3-Class Distribution:\")\n","    class_dist = uss_reviews['true_label'].value_counts().sort_index()\n","    class_names = ['Negative (‚â§1‚òÖ)', 'Neutral (2‚òÖ)', 'Positive (‚â•3‚òÖ)']\n","    for i, count in enumerate(class_dist):\n","        print(f\"  {class_names[i]}: {count} ({count/len(uss_reviews)*100:.1f}%)\")\n","\n","    # Load trained model\n","    try:\n","        model, vocab = load_trained_model(model_path)\n","    except FileNotFoundError:\n","        print(f\"‚ùå Error: Could not find model file '{model_path}'\")\n","        print(\"Please make sure the model has been trained and saved.\")\n","        return\n","\n","    # Make predictions\n","    print(f\"\\nMaking predictions on {len(uss_reviews)} reviews...\")\n","    start_time = time.time()\n","\n","    texts = uss_reviews['review'].tolist()\n","    true_labels = uss_reviews['true_label'].tolist()\n","\n","    predictions, probabilities = predict_batch(model, texts, vocab)\n","\n","    inference_time = time.time() - start_time\n","    print(f\"‚úì Inference completed in {inference_time:.2f} seconds\")\n","    print(f\"‚úì Average time per review: {inference_time/len(texts)*1000:.2f}ms\")\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(true_labels, predictions)\n","    macro_f1 = f1_score(true_labels, predictions, average='macro')\n","    weighted_f1 = f1_score(true_labels, predictions, average='weighted')\n","\n","    # Print results\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"üéØ EVALUATION RESULTS\")\n","    print(\"=\"*60)\n","    print(f\"Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n","    print(f\"Macro F1-Score: {macro_f1:.4f}\")\n","    print(f\"Weighted F1-Score: {weighted_f1:.4f}\")\n","\n","    # Detailed classification report\n","    print(f\"\\nüìä DETAILED CLASSIFICATION REPORT:\")\n","    print(\"-\" * 50)\n","    target_names = ['Negative', 'Neutral', 'Positive']\n","    report = classification_report(true_labels, predictions, target_names=target_names, digits=4)\n","    print(report)\n","\n","    # Confusion matrix\n","    print(f\"\\nüîÑ CONFUSION MATRIX:\")\n","    print(\"-\" * 30)\n","    cm = confusion_matrix(true_labels, predictions)\n","    print(\"         Predicted\")\n","    print(\"       Neg  Neu  Pos\")\n","    print(\"Actual\")\n","    for i, row in enumerate(cm):\n","        class_name = ['Neg', 'Neu', 'Pos'][i]\n","        print(f\"  {class_name}  {row[0]:4d} {row[1]:4d} {row[2]:4d}\")\n","\n","    # Per-class accuracy\n","    print(f\"\\nüìà PER-CLASS PERFORMANCE:\")\n","    print(\"-\" * 40)\n","    for i, class_name in enumerate(target_names):\n","        class_mask = np.array(true_labels) == i\n","        if class_mask.sum() > 0:\n","            class_acc = (np.array(predictions)[class_mask] == i).mean()\n","            class_count = class_mask.sum()\n","            print(f\"{class_name:>8}: {class_acc:.4f} accuracy ({class_count:,} samples)\")\n","\n","    # Save detailed results\n","    print(f\"\\nüíæ SAVING DETAILED RESULTS...\")\n","\n","    # Add predictions to dataframe\n","    uss_reviews['predicted_label'] = predictions\n","    uss_reviews['predicted_class'] = [target_names[pred] for pred in predictions]\n","    uss_reviews['true_class'] = [target_names[true] for true in true_labels]\n","    uss_reviews['confidence'] = probabilities.max(axis=1)\n","    uss_reviews['prob_negative'] = probabilities[:, 0]\n","    uss_reviews['prob_neutral'] = probabilities[:, 1]\n","    uss_reviews['prob_positive'] = probabilities[:, 2]\n","    uss_reviews['correct_prediction'] = uss_reviews['true_label'] == uss_reviews['predicted_label']\n","\n","    # Save results\n","    output_file = 'uss_reviews_predictions.csv'\n","    uss_reviews.to_csv(output_file, index=False)\n","    print(f\"‚úì Results saved to '{output_file}'\")\n","\n","    # Show some example predictions\n","    print(f\"\\nüîç EXAMPLE PREDICTIONS:\")\n","    print(\"-\" * 50)\n","\n","    # Show some correct and incorrect predictions\n","    correct_preds = uss_reviews[uss_reviews['correct_prediction'] == True].sample(n=min(3, len(uss_reviews)), random_state=42)\n","    incorrect_preds = uss_reviews[uss_reviews['correct_prediction'] == False].sample(n=min(2, (uss_reviews['correct_prediction'] == False).sum()), random_state=42)\n","\n","    print(\"‚úÖ CORRECT PREDICTIONS:\")\n","    for idx, row in correct_preds.iterrows():\n","        print(f\"Review: {row['review'][:100]}...\")\n","        print(f\"Stars: {row['stars']} ‚Üí True: {row['true_class']} | Predicted: {row['predicted_class']} | Confidence: {row['confidence']:.3f}\")\n","        print()\n","\n","    if len(incorrect_preds) > 0:\n","        print(\"‚ùå INCORRECT PREDICTIONS:\")\n","        for idx, row in incorrect_preds.iterrows():\n","            print(f\"Review: {row['review'][:100]}...\")\n","            print(f\"Stars: {row['stars']} ‚Üí True: {row['true_class']} | Predicted: {row['predicted_class']} | Confidence: {row['confidence']:.3f}\")\n","            print()\n","\n","    print(\"=\"*60)\n","    print(\"üéâ INFERENCE COMPLETE!\")\n","    print(\"=\"*60)\n","\n","    return uss_reviews, accuracy, macro_f1\n","\n","# Custom prediction function for new reviews\n","def predict_new_review(review_text, model_path='model/3class_transformer_v2.pth'):\n","    \"\"\"Predict sentiment for a single new review\"\"\"\n","    model, vocab = load_trained_model(model_path)\n","\n","    predictions, probabilities = predict_batch(model, [review_text], vocab, batch_size=1)\n","\n","    class_names = ['Negative', 'Neutral', 'Positive']\n","    predicted_class = predictions[0]\n","    confidence = probabilities[0][predicted_class]\n","\n","    print(f\"Review: {review_text}\")\n","    print(f\"Predicted: {class_names[predicted_class]} (confidence: {confidence:.3f})\")\n","    print(f\"Probabilities - Neg: {probabilities[0][0]:.3f}, Neu: {probabilities[0][1]:.3f}, Pos: {probabilities[0][2]:.3f}\")\n","\n","    return class_names[predicted_class], confidence, probabilities[0]\n","\n","if __name__ == \"__main__\":\n","    # Run inference on USS Reviews\n","    results_df, accuracy, macro_f1 = evaluate_on_uss_reviews()\n","\n","    # Example of predicting a new review\n","    print(f\"\\n\" + \"=\"*60)\n","    print(\"üîÆ TESTING ON NEW REVIEW:\")\n","    print(\"=\"*60)\n","\n","    sample_review = \"The rides were amazing and the staff was super friendly! Had a great time with family.\"\n","    predict_new_review(sample_review)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i4IceY0zt6wt","executionInfo":{"status":"ok","timestamp":1750338689804,"user_tz":-480,"elapsed":38260,"user":{"displayName":"Kindness Matters","userId":"04525017512802736418"}},"outputId":"12f817ec-5fb9-44ce-d788-24bee0264271"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","============================================================\n","üé¨ USS REVIEWS SENTIMENT ANALYSIS\n","============================================================\n","Loading USS Reviews dataset...\n","‚úì Loaded 29412 reviews\n","Preprocessing data...\n","‚úì Cleaned data: 29412 reviews (0 removed)\n","\n","USS Reviews Star Distribution:\n","stars\n","1     1370\n","2      836\n","3     2133\n","4     5558\n","5    19515\n","Name: count, dtype: int64\n","\n","USS Reviews 3-Class Distribution:\n","  Negative (‚â§1‚òÖ): 2206 (7.5%)\n","  Neutral (2‚òÖ): 2133 (7.3%)\n","  Positive (‚â•3‚òÖ): 25073 (85.2%)\n","Loading model from model/3class_transformer_v2.pth...\n","‚úì Model loaded successfully!\n","‚úì Vocabulary size: 12000\n","‚úì Model parameters: 2,420,099\n","\n","Making predictions on 29412 reviews...\n","‚úì Inference completed in 37.11 seconds\n","‚úì Average time per review: 1.26ms\n","\n","============================================================\n","üéØ EVALUATION RESULTS\n","============================================================\n","Overall Accuracy: 0.6868 (68.68%)\n","Macro F1-Score: 0.4998\n","Weighted F1-Score: 0.7451\n","\n","üìä DETAILED CLASSIFICATION REPORT:\n","--------------------------------------------------\n","              precision    recall  f1-score   support\n","\n","    Negative     0.3380    0.8096    0.4769      2206\n","     Neutral     0.1405    0.4004    0.2080      2133\n","    Positive     0.9729    0.7004    0.8144     25073\n","\n","    accuracy                         0.6868     29412\n","   macro avg     0.4838    0.6368    0.4998     29412\n","weighted avg     0.8649    0.6868    0.7451     29412\n","\n","\n","üîÑ CONFUSION MATRIX:\n","------------------------------\n","         Predicted\n","       Neg  Neu  Pos\n","Actual\n","  Neg  1786  288  132\n","  Neu   922  854  357\n","  Pos  2576 4937 17560\n","\n","üìà PER-CLASS PERFORMANCE:\n","----------------------------------------\n","Negative: 0.8096 accuracy (2,206 samples)\n"," Neutral: 0.4004 accuracy (2,133 samples)\n","Positive: 0.7004 accuracy (25,073 samples)\n","\n","üíæ SAVING DETAILED RESULTS...\n","‚úì Results saved to 'uss_reviews_predictions.csv'\n","\n","üîç EXAMPLE PREDICTIONS:\n","--------------------------------------------------\n","‚úÖ CORRECT PREDICTIONS:\n","Review: very good...\n","Stars: 5 ‚Üí True: Positive | Predicted: Positive | Confidence: 0.867\n","\n","Review: Love it!!!...\n","Stars: 5 ‚Üí True: Positive | Predicted: Positive | Confidence: 0.928\n","\n","Review: Our experience here was great! They have 12-18 rides. Each ride will only take 3-5mins, but the wait...\n","Stars: 5 ‚Üí True: Positive | Predicted: Positive | Confidence: 0.518\n","\n","‚ùå INCORRECT PREDICTIONS:\n","Review: Nice theme park for everyone...\n","Stars: 5 ‚Üí True: Positive | Predicted: Neutral | Confidence: 0.621\n","\n","Review: It's empty on weekdays...\n","Stars: 5 ‚Üí True: Positive | Predicted: Neutral | Confidence: 0.656\n","\n","============================================================\n","üéâ INFERENCE COMPLETE!\n","============================================================\n","\n","============================================================\n","üîÆ TESTING ON NEW REVIEW:\n","============================================================\n","Loading model from model/3class_transformer_v2.pth...\n","‚úì Model loaded successfully!\n","‚úì Vocabulary size: 12000\n","‚úì Model parameters: 2,420,099\n","Review: The rides were amazing and the staff was super friendly! Had a great time with family.\n","Predicted: Positive (confidence: 0.927)\n","Probabilities - Neg: 0.014, Neu: 0.059, Pos: 0.927\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyOy5yku1L8/6Mlfjg8icCdD"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}