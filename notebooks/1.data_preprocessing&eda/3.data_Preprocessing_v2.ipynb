{"cells":[{"cell_type":"markdown","metadata":{"id":"QNSLK3ra2e2X"},"source":["# Data Preprocessing Pipeline\n","\n","| Step                          | Purpose                                                      | Method/Components Used |\n","| ----------------------------- | ------------------------------------------------------------ | ---------------------- |\n","| **Imports**                   | Load required libraries for ML, NLP, and data processing    | pandas, numpy, sklearn, xgboost, sentence-transformers, spacy, transformers, imbalanced-learn |\n","| **Load Data**                 | Read CSV file and identify text/ID columns                  | `pd.read_csv()`, column validation, data type conversion |\n","| **Model Initialization**      | Load pre-trained NLP models and pipelines                   | spaCy (en_core_web_sm), SentenceTransformer (all-MiniLM-L6-v2), RoBERTa sentiment |\n","| **Text Preprocessing**        | Clean and normalize review text                              | Regex patterns, whitespace normalization, punctuation cleanup |\n","| **Label Generation**          | Create spam labels using heuristic scoring                  | Length checks, repetition analysis, pattern matching, contextual scoring |\n","| **Feature Extraction**        | Generate comprehensive feature vectors                       | **TF-IDF** (300 features, 1-2 grams), **Sentence embeddings** (384-dim), **Linguistic features** (POS, entities), **Style features** (ratios, patterns) |\n","| **Train/Test Split**          | Split dataset for model validation                          | `train_test_split()` with stratification (80/20 split) |\n","| **Feature Scaling**           | Normalize features for consistent model input               | `StandardScaler()` fit on training data |\n","| **Class Balancing**           | Address class imbalance in training data                    | SMOTE oversampling with random_state=42 |\n","| **Model Training**            | Train ensemble classifier                                    | **Random Forest** (100 estimators, balanced) + **XGBoost** (auto-scaled) with soft voting |\n","| **Model Evaluation**          | Assess classifier performance                                | Confusion matrix, classification report, accuracy score, F1-score |\n","| **Spam Prediction**           | Generate spam probabilities for all reviews                 | Ensemble `.predict_proba()` with 0.5 threshold |\n","| **Duplicate Detection**       | Identify near-identical reviews using clustering            | **DBSCAN** (eps=0.01, cosine similarity) + text verification |\n","| **Off-topic Detection**       | Flag reviews lacking domain relevance                       | Multi-criteria filtering: length + keyword analysis + review language detection |\n","| **Informativeness Scoring**   | Calculate content quality metrics                           | **Formula**: 0.4Ã—lexical_diversity + 0.3Ã—entity_density + 0.3Ã—length_factor |\n","| **Results Integration**       | Combine all analysis flags and scores                       | Create comprehensive DataFrame with all detection results |\n","| **Quality Filtering**         | Apply conservative filtering rules                          | Remove high-confidence spam (>0.8) OR duplicates OR multi-issue reviews |\n","| **Export Results**            | Save processed datasets to CSV files                       | **Reviews_Clean.csv** (filtered), **Reviews_Filtered.csv** (removed), **Reviews_Analysis.csv** (complete) |\n","\n","## Pipeline Flow Summary\n","\n","```\n","Input CSV â†’ Model Loading â†’ Text Preprocessing â†’ Feature Engineering â†’\n","ML Training â†’ Multi-Detection â†’ Quality Assessment â†’ Output Generation\n","```\n","\n","**Key Metrics Tracked:**\n","- Original reviews count\n","- Clean reviews retained (%)\n","- Spam detected (%)\n","- Duplicates found (%)\n","- Off-topic flagged (%)\n","- Average informativeness score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"z9BEcIJjAOJE"},"outputs":[],"source":["#@title connect google drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/SMU_MITB_NLP/Project/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SqZOVDZGyxsD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751304019916,"user_tz":-480,"elapsed":136741,"user":{"displayName":"CHENG LE","userId":"15930612432530710348"}},"outputId":"c1d1d721-e66a-4c8e-9e1b-7838ddbae4ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n","Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n","Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","Collecting en-core-web-sm==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}],"source":["!pip install pandas scikit-learn xgboost sentence-transformers transformers tqdm imbalanced-learn spacy\n","!python -m spacy download en_core_web_sm\n","!pip install textstat"]},{"cell_type":"markdown","source":["## Overview\n","\n","This is a comprehensive machine learning-based system for filtering and analyzing product reviews to identify spam, duplicates, off-topic content, and measure informativeness. The system uses ensemble methods, natural language processing, and heuristic approaches to clean review datasets.\n","\n","## Key Features\n","\n","### ğŸ” **Multi-Modal Detection**\n","- **Spam Detection**: ML-based classification using ensemble methods\n","- **Duplicate Detection**: Clustering-based approach with similarity thresholds\n","- **Off-Topic Detection**: Domain-specific keyword analysis\n","- **Informativeness Scoring**: Content richness evaluation\n","\n","### ğŸ¤– **Machine Learning Components**\n","- **Ensemble Classifier**: Random Forest + XGBoost with soft voting\n","- **Feature Engineering**: TF-IDF, sentence embeddings, linguistic features, style features\n","- **Class Balancing**: SMOTE oversampling for imbalanced datasets\n","- **Cross-Validation**: Stratified train/test splits\n","\n","### ğŸ“Š **NLP Processing**\n","- **Sentence Transformers**: all-MiniLM-L6-v2 for semantic embeddings\n","- **spaCy Integration**: Linguistic feature extraction (POS tags, entities)\n","- **Sentiment Analysis**: Twitter-RoBERTa model integration\n","- **Readability Analysis**: Flesch reading ease scoring\n","\n","## System Architecture\n","\n","### Feature Extraction Pipeline\n","\n","#### 1. **TF-IDF Features**\n","```python\n","TfidfVectorizer(\n","    max_features=300,\n","    ngram_range=(1, 2),\n","    min_df=2,\n","    max_df=0.95,\n","    stop_words='english'\n",")\n","```\n","\n","#### 2. **Sentence Embeddings**\n","- Model: `all-MiniLM-L6-v2`\n","- Generates 384-dimensional semantic vectors\n","- Batch processing for efficiency\n","\n","#### 3. **Linguistic Features**\n","- Token counts and sentence segmentation\n","- POS tag ratios (NOUN, VERB, ADJ)\n","- Named entity density\n","- Average words per sentence\n","\n","#### 4. **Style Features**\n","- Character/word count ratios\n","- Capitalization and punctuation patterns\n","- Lexical diversity scores\n","- URL detection\n","- Sentiment scores\n","- Readability metrics\n","\n","## Detection Algorithms\n","\n","### Spam Detection\n","\n","**Heuristic Scoring System:**\n","- Length-based penalties (< 8 characters)\n","- Word repetition analysis\n","- Pattern matching (excessive caps, punctuation)\n","- Context-aware scoring (star ratings correlation)\n","\n","**Threshold:** Spam score â‰¥ 2\n","\n","### Duplicate Detection\n","\n","**DBSCAN Clustering:**\n","- Epsilon: 0.01 (strict similarity threshold)\n","- Min samples: 2\n","- Cosine similarity metric\n","- Additional text verification for high precision\n","\n","### Off-Topic Detection\n","\n","**Multi-Criteria Filtering:**\n","Only flagged when ALL conditions met:\n","- Very short content (< 10 characters)\n","- No domain-relevant keywords\n","- No review-specific language\n","- No emotional indicators\n","- No evaluative content\n","\n","### Informativeness Scoring\n","\n","**Composite Score Formula:**\n","```\n","info_score = 0.4 Ã— lexical_diversity +\n","             0.3 Ã— entity_density +\n","             0.3 Ã— length_factor\n","```"],"metadata":{"id":"Go6ka1BP2_Um"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import re\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Core ML libraries\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from xgboost import XGBClassifier\n","from imblearn.over_sampling import SMOTE\n","\n","# NLP libraries\n","from sentence_transformers import SentenceTransformer\n","from transformers import pipeline\n","import spacy\n","from textstat import flesch_reading_ease\n","from sklearn.cluster import DBSCAN\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","class CompactReviewFilter:\n","    def __init__(self, csv_path, text_col='review', id_col='review_index'):\n","        \"\"\"Initialize the review filter with essential components.\"\"\"\n","        self.df = pd.read_csv(csv_path)\n","        self.texts = self.df[text_col].astype(str).tolist()\n","        self.text_col = text_col\n","        self.id_col = id_col if id_col in self.df.columns else None\n","\n","        print(f\"Loaded {len(self.texts)} reviews\")\n","        print(f\"Available columns: {list(self.df.columns)}\")\n","\n","        # Initialize models\n","        print(\"Loading NLP models...\")\n","        try:\n","            self.nlp = spacy.load(\"en_core_web_sm\")\n","        except:\n","            print(\"spaCy model not found. Install with: python -m spacy download en_core_web_sm\")\n","            raise\n","\n","        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n","        try:\n","            self.sentiment_model = pipeline(\"sentiment-analysis\",\n","                                           model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n","        except:\n","            self.sentiment_model = None\n","            print(\"Sentiment model not available\")\n","\n","    def preprocess_texts(self):\n","        \"\"\"Basic text preprocessing.\"\"\"\n","        processed = []\n","        for text in self.texts:\n","            # Clean whitespace and basic normalization\n","            text = re.sub(r'\\s+', ' ', str(text).strip())\n","            text = re.sub(r'([.!?])\\1+', r'\\1', text)  # Remove repeated punctuation\n","            processed.append(text)\n","        return processed\n","\n","    def extract_features(self):\n","        \"\"\"Extract key features for classification.\"\"\"\n","        print(\"Extracting features...\")\n","\n","        # Preprocess texts\n","        clean_texts = self.preprocess_texts()\n","\n","        # 1. TF-IDF features\n","        tfidf = TfidfVectorizer(max_features=300, ngram_range=(1, 2),\n","                               min_df=2, max_df=0.95, stop_words='english')\n","        X_tfidf = tfidf.fit_transform(clean_texts).toarray()\n","\n","        # 2. Sentence embeddings\n","        X_embeddings = self.sentence_model.encode(clean_texts, show_progress_bar=True, batch_size=32)\n","\n","        # 3. Basic linguistic features\n","        X_linguistic = self._get_linguistic_features(clean_texts)\n","\n","        # 4. Style features\n","        X_style = self._get_style_features(clean_texts)\n","\n","        # Combine features\n","        X_combined = np.hstack([X_tfidf, X_embeddings, X_linguistic, X_style])\n","        print(f\"Total features: {X_combined.shape[1]}\")\n","\n","        return X_combined\n","\n","    def _get_linguistic_features(self, texts):\n","        \"\"\"Extract linguistic features using spaCy.\"\"\"\n","        features = []\n","        for text in tqdm(texts, desc=\"Linguistic features\"):\n","            doc = self.nlp(text)\n","\n","            num_tokens = len([t for t in doc if not t.is_punct])\n","            num_sentences = len(list(doc.sents))\n","            num_words = len([t for t in doc if t.is_alpha])\n","\n","            # POS ratios\n","            pos_counts = {}\n","            for token in doc:\n","                if not token.is_punct:\n","                    pos_counts[token.pos_] = pos_counts.get(token.pos_, 0) + 1\n","            total_pos = sum(pos_counts.values()) or 1\n","\n","            # Named entities\n","            num_entities = len(doc.ents)\n","\n","            feature_vec = [\n","                num_tokens,\n","                num_sentences,\n","                num_words / max(num_sentences, 1),  # avg words per sentence\n","                pos_counts.get('NOUN', 0) / total_pos,\n","                pos_counts.get('VERB', 0) / total_pos,\n","                pos_counts.get('ADJ', 0) / total_pos,\n","                num_entities / max(num_tokens, 1),  # entity density\n","            ]\n","            features.append(feature_vec)\n","\n","        return np.array(features)\n","\n","    def _get_style_features(self, texts):\n","        \"\"\"Extract style and quality features.\"\"\"\n","        features = []\n","        for text in tqdm(texts, desc=\"Style features\"):\n","            # Basic counts\n","            char_count = len(text)\n","            words = text.split()\n","            word_count = len(words)\n","\n","            # Ratios\n","            upper_ratio = sum(1 for c in text if c.isupper()) / max(char_count, 1)\n","            digit_ratio = sum(1 for c in text if c.isdigit()) / max(char_count, 1)\n","            punct_ratio = sum(1 for c in text if c in '.,!?;:') / max(char_count, 1)\n","\n","            # Word-level analysis\n","            avg_word_len = np.mean([len(w) for w in words]) if words else 0\n","            unique_words = len(set(words))\n","            lexical_diversity = unique_words / max(word_count, 1)\n","\n","            # Pattern detection\n","            has_urls = 1 if re.search(r'http|www\\.', text) else 0\n","            excessive_caps = 1 if upper_ratio > 0.3 else 0\n","            excessive_punct = 1 if punct_ratio > 0.15 else 0\n","\n","            # Sentiment if available\n","            sentiment_score = 0\n","            if self.sentiment_model:\n","                try:\n","                    result = self.sentiment_model(text[:512])\n","                    if isinstance(result, list) and len(result) > 0:\n","                        sentiment_score = result[0].get('score', 0)\n","                except:\n","                    sentiment_score = 0\n","\n","            # Readability\n","            try:\n","                readability = flesch_reading_ease(text) / 100\n","            except:\n","                readability = 0.5\n","\n","            feature_vec = [\n","                char_count / 500,  # normalized\n","                upper_ratio,\n","                digit_ratio,\n","                punct_ratio,\n","                avg_word_len / 8,  # normalized\n","                lexical_diversity,\n","                has_urls,\n","                excessive_caps,\n","                excessive_punct,\n","                sentiment_score,\n","                readability\n","            ]\n","            features.append(feature_vec)\n","\n","        return np.array(features)\n","\n","    def generate_labels(self):\n","        \"\"\"Generate spam labels using improved heuristics.\"\"\"\n","        print(\"Generating spam labels...\")\n","        labels = []\n","\n","        for i, text in enumerate(self.texts):\n","            spam_score = 0\n","            text_clean = str(text).strip().lower()\n","            words = text_clean.split()\n","\n","            # Length checks\n","            if len(text_clean) < 8:\n","                spam_score += 2\n","            elif len(words) < 3:\n","                spam_score += 1\n","\n","            # Repetition\n","            if len(words) > 0:\n","                unique_ratio = len(set(words)) / len(words)\n","                if unique_ratio < 0.5:\n","                    spam_score += 1\n","\n","            # Pattern checks\n","            if re.search(r'[A-Z]{6,}', text):  # Excessive caps\n","                spam_score += 1\n","            if re.search(r'[!?]{3,}', text):  # Excessive punctuation\n","                spam_score += 1\n","            if re.search(r'(.)\\1{4,}', text):  # Character repetition\n","                spam_score += 1\n","\n","            # Common spam patterns\n","            if re.match(r'^(good|bad|ok|great|nice|test)$', text_clean):\n","                spam_score += 1\n","\n","            # Star rating context (if available)\n","            if 'stars' in self.df.columns:\n","                try:\n","                    stars = float(self.df.iloc[i]['stars'])\n","                    if (stars == 5 or stars == 1) and len(text_clean) < 10:\n","                        spam_score += 1\n","                except:\n","                    pass\n","\n","            labels.append(1 if spam_score >= 2 else 0)\n","\n","        spam_count = sum(labels)\n","        print(f\"Generated labels: {spam_count} spam ({spam_count/len(labels):.1%}) out of {len(labels)}\")\n","        return np.array(labels)\n","\n","    def train_classifier(self, X, y):\n","        \"\"\"Train spam classifier.\"\"\"\n","        print(\"Training classifier...\")\n","\n","        # Split data\n","        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","        # Scale features\n","        scaler = StandardScaler()\n","        X_train_scaled = scaler.fit_transform(X_train)\n","        X_test_scaled = scaler.transform(X_test)\n","\n","        # Balance with SMOTE\n","        smote = SMOTE(random_state=42)\n","        X_train_bal, y_train_bal = smote.fit_resample(X_train_scaled, y_train)\n","\n","        print(f\"Training samples: {len(y_train_bal)} (spam: {sum(y_train_bal)}, clean: {sum(y_train_bal==0)})\")\n","\n","        # Train ensemble\n","        rf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42, n_jobs=-1)\n","        xgb = XGBClassifier(scale_pos_weight=sum(y_train_bal==0)/sum(y_train_bal==1),\n","                           use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1)\n","\n","        ensemble = VotingClassifier([('rf', rf), ('xgb', xgb)], voting='soft')\n","        ensemble.fit(X_train_bal, y_train_bal)\n","\n","        # Evaluate\n","        y_pred = ensemble.predict(X_test_scaled)\n","        y_probs = ensemble.predict_proba(X_test_scaled)[:, 1]\n","\n","        print(\"\\n=== MODEL EVALUATION ===\")\n","        print(confusion_matrix(y_test, y_pred))\n","        print(classification_report(y_test, y_pred))\n","        print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n","\n","        # Store for later use\n","        self.scaler = scaler\n","        self.ensemble = ensemble\n","\n","        return ensemble\n","\n","    def detect_duplicates(self, X):\n","        \"\"\"Detect duplicate reviews with more conservative thresholds.\"\"\"\n","        print(\"Detecting duplicates...\")\n","\n","        # Use much more conservative parameters for DBSCAN\n","        dbscan = DBSCAN(eps=0.01, min_samples=2, metric='cosine', n_jobs=-1)  # Much stricter similarity\n","        clusters = dbscan.fit_predict(X)\n","\n","        dup_flags = [False] * len(self.texts)\n","        dup_refs = [None] * len(self.texts)\n","\n","        # Only mark as duplicate if texts are very similar\n","        for cluster_id in set(clusters):\n","            if cluster_id == -1:  # Skip noise\n","                continue\n","            cluster_indices = np.where(clusters == cluster_id)[0]\n","            if len(cluster_indices) > 1:\n","                # Additional check: verify actual text similarity\n","                cluster_texts = [self.texts[i] for i in cluster_indices]\n","                verified_duplicates = []\n","\n","                for i, idx in enumerate(cluster_indices):\n","                    text1 = self.texts[idx].lower().strip()\n","                    # Check if this text is very similar to any already verified\n","                    is_dup = False\n","                    for base_idx in verified_duplicates:\n","                        text2 = self.texts[base_idx].lower().strip()\n","                        # Check for exact or near-exact matches\n","                        if (text1 == text2 or\n","                            len(text1) > 20 and len(text2) > 20 and\n","                            abs(len(text1) - len(text2)) < 5 and\n","                            sum(c1 == c2 for c1, c2 in zip(text1, text2)) / max(len(text1), len(text2)) > 0.9):\n","                            dup_flags[idx] = True\n","                            dup_refs[idx] = base_idx\n","                            is_dup = True\n","                            break\n","\n","                    if not is_dup:\n","                        verified_duplicates.append(idx)\n","\n","        dup_count = sum(dup_flags)\n","        print(f\"Duplicates found: {dup_count} ({dup_count/len(self.texts):.1%})\")\n","        return dup_flags, dup_refs\n","\n","    def detect_off_topic(self):\n","        \"\"\"Detect off-topic reviews with more lenient criteria.\"\"\"\n","        print(\"Detecting off-topic reviews...\")\n","\n","        # Expanded domain keywords\n","        domain_words = ['product', 'service', 'quality', 'delivery', 'shipping', 'price', 'order',\n","                       'buy', 'purchase', 'recommend', 'good', 'bad', 'great', 'terrible', 'awesome',\n","                       'satisfied', 'disappointed', 'excellent', 'poor', 'fast', 'slow', 'cheap',\n","                       'expensive', 'worth', 'value', 'money', 'item', 'received', 'arrived',\n","                       'package', 'box', 'customer', 'seller', 'store', 'shop', 'website',\n","                       'online', 'return', 'refund', 'exchange', 'warranty', 'brand', 'company']\n","\n","        # Review-specific language\n","        review_words = ['recommend', 'bought', 'ordered', 'received', 'love', 'hate', 'like',\n","                       'dislike', 'amazing', 'terrible', 'perfect', 'awful', 'satisfied',\n","                       'disappointed', 'happy', 'unhappy', 'pleased', 'upset', 'impressed',\n","                       'expected', 'surprised', 'works', 'broken', 'defective', 'damaged']\n","\n","        # Emotional/evaluative words\n","        emotion_words = ['love', 'hate', 'like', 'dislike', 'happy', 'sad', 'angry', 'excited',\n","                        'disappointed', 'satisfied', 'pleased', 'upset', 'glad', 'sorry']\n","\n","        flags = []\n","        for text in self.texts:\n","            text_lower = str(text).lower().strip()\n","            words = text_lower.split()\n","\n","            # Only mark as off-topic if ALL conditions are met:\n","            # 1. Very short (less than 10 characters)\n","            # 2. No domain words\n","            # 3. No review language\n","            # 4. No emotional content\n","\n","            is_very_short = len(text_lower) < 10\n","            has_domain_words = any(word in text_lower for word in domain_words)\n","            has_review_lang = any(word in text_lower for word in review_words)\n","            has_emotions = any(word in text_lower for word in emotion_words)\n","\n","            # Also check for basic review structure\n","            has_evaluation = any(word in text_lower for word in ['good', 'bad', 'ok', 'okay', 'fine', 'nice'])\n","\n","            # Only mark as off-topic if it's very short AND lacks any review indicators\n","            is_off_topic = (is_very_short and\n","                           not has_domain_words and\n","                           not has_review_lang and\n","                           not has_emotions and\n","                           not has_evaluation)\n","\n","            flags.append(is_off_topic)\n","\n","        off_topic_count = sum(flags)\n","        print(f\"Off-topic found: {off_topic_count} ({off_topic_count/len(flags):.1%})\")\n","        return flags\n","\n","    def calculate_informativeness(self):\n","        \"\"\"Calculate informativeness scores.\"\"\"\n","        print(\"Calculating informativeness...\")\n","\n","        scores = []\n","        for text in self.texts:\n","            doc = self.nlp(str(text))\n","\n","            # Basic metrics\n","            num_words = len([t for t in doc if t.is_alpha])\n","            if num_words == 0:\n","                scores.append(0.0)\n","                continue\n","\n","            # Lexical diversity\n","            words = [t.lower_ for t in doc if t.is_alpha]\n","            lexical_diversity = len(set(words)) / len(words)\n","\n","            # Entities and content richness\n","            entities = len(doc.ents)\n","            entity_density = entities / num_words\n","\n","            # Length bonus\n","            length_factor = min(num_words / 20, 1.0)\n","\n","            # Combine factors\n","            info_score = (0.4 * lexical_diversity +\n","                         0.3 * min(entity_density * 10, 1) +\n","                         0.3 * length_factor)\n","\n","            scores.append(round(min(1.0, max(0.0, info_score)), 3))\n","\n","        return scores\n","\n","    def run_analysis(self):\n","        \"\"\"Run complete analysis pipeline.\"\"\"\n","        print(\"=== STARTING REVIEW ANALYSIS ===\\n\")\n","\n","        # Extract features\n","        X = self.extract_features()\n","\n","        # Generate labels and train classifier\n","        y = self.generate_labels()\n","        model = self.train_classifier(X, y)\n","\n","        # Get spam predictions\n","        X_scaled = self.scaler.transform(X)\n","        spam_probs = self.ensemble.predict_proba(X_scaled)[:, 1]\n","        spam_pred = (spam_probs > 0.5).astype(int)\n","\n","        # Detect other issues\n","        dup_flags, dup_refs = self.detect_duplicates(X)\n","        off_topic_flags = self.detect_off_topic()\n","        info_scores = self.calculate_informativeness()\n","\n","        # Create results dataframe\n","        results = pd.DataFrame({\n","            'review_id': range(len(self.texts)) if not self.id_col else self.df[self.id_col],\n","            'review': self.texts,\n","            'is_spam': spam_pred,\n","            'spam_probability': np.round(spam_probs, 4),\n","            'is_duplicate': dup_flags,\n","            'duplicate_of': dup_refs,\n","            'is_off_topic': off_topic_flags,\n","            'informativeness_score': info_scores\n","        })\n","\n","        # Create clean and filtered datasets with more balanced approach\n","        # Only remove if multiple issues are present OR very high confidence single issue\n","        problematic_mask = (\n","            (results['is_spam'] & (results['spam_probability'] > 0.8)) |  # High confidence spam\n","            (results['is_duplicate']) |  # Keep duplicate detection as is\n","            (results['is_off_topic'] & results['is_spam']) |  # Off-topic AND spam\n","            (results['is_off_topic'] & (results['informativeness_score'] < 0.1))  # Off-topic AND low info\n","        )\n","\n","        keep_mask = ~problematic_mask\n","\n","        clean_df = self.df[keep_mask].copy()\n","        filtered_df = self.df[problematic_mask].copy()\n","\n","        # Add analysis columns to filtered dataset\n","        for col in ['is_spam', 'spam_probability', 'is_duplicate', 'duplicate_of',\n","                   'is_off_topic', 'informativeness_score']:\n","            filtered_df[col] = results[col][problematic_mask].values\n","\n","        # Save results\n","        clean_df.to_csv(\"Reviews_Clean.csv\", index=False)\n","        filtered_df.to_csv(\"Reviews_Filtered.csv\", index=False)\n","        results.to_csv(\"Reviews_Analysis.csv\", index=False)\n","\n","        # Summary\n","        print(f\"\\n=== ANALYSIS SUMMARY ===\")\n","        print(f\"Original reviews: {len(self.df)}\")\n","        print(f\"Clean reviews: {len(clean_df)} ({len(clean_df)/len(self.df):.1%})\")\n","        print(f\"Filtered out: {len(filtered_df)} ({len(filtered_df)/len(self.df):.1%})\")\n","        print(f\"  - Spam: {sum(results['is_spam'])} ({sum(results['is_spam'])/len(self.df):.1%})\")\n","        print(f\"  - Duplicates: {sum(results['is_duplicate'])} ({sum(results['is_duplicate'])/len(self.df):.1%})\")\n","        print(f\"  - Off-topic: {sum(results['is_off_topic'])} ({sum(results['is_off_topic'])/len(self.df):.1%})\")\n","\n","        return clean_df, filtered_df, results\n","\n","# Usage\n","if __name__ == \"__main__\":\n","    # Initialize and run analysis\n","    filter_system = CompactReviewFilter(\n","        csv_path='USS_Reviews_Silver.csv',\n","        text_col='review',\n","        id_col='review_index'\n","    )\n","\n","    clean_reviews, filtered_reviews, analysis_results = filter_system.run_analysis()\n","\n","    print(f\"\\nFiles saved:\")\n","    print(f\"- Reviews_Clean.csv: {len(clean_reviews)} clean reviews\")\n","    print(f\"- Reviews_Filtered.csv: {len(filtered_reviews)} filtered reviews\")\n","    print(f\"- Reviews_Analysis.csv: Complete analysis results\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":900,"referenced_widgets":["9e32807844de495eb040d1985680ca26","4860235e612c4331b3e2d408e6486bee","ee325664fe6945d59915f33e38499ebf","6081529b2ca344348e899ba4b30e1e67","06464759abad4fdd81cdc406f573ac06","9611e213a0cb443b99c0ebf8835bcc78","b6733cadaee347139a05b5e43bdcd747","0ab1c40569004ca69ca24a32f34f33ef","3463e8064f89475a98313d7a56493aa8","5486e712ee2b48f48886b49a8fca6e35","e0422e1097e2428e8d68d28df18f4333"]},"id":"Gl6K820DWjEg","executionInfo":{"status":"ok","timestamp":1750525334104,"user_tz":-480,"elapsed":1472499,"user":{"displayName":"CHENG LE","userId":"15930612432530710348"}},"outputId":"108c613c-cc56-4d68-d19b-7367cd855e1f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 29412 reviews\n","Available columns: ['integrated_review', 'stars', 'name', 'review', 'publishedAtDate', 'data_split', 'review_index']\n","Loading NLP models...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Device set to use cuda:0\n"]},{"output_type":"stream","name":"stdout","text":["=== STARTING REVIEW ANALYSIS ===\n","\n","Extracting features...\n"]},{"output_type":"display_data","data":{"text/plain":["Batches:   0%|          | 0/920 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e32807844de495eb040d1985680ca26"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Linguistic features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29412/29412 [04:44<00:00, 103.33it/s]\n","Style features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29412/29412 [04:43<00:00, 103.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Total features: 702\n","Generating spam labels...\n","Generated labels: 1215 spam (4.1%) out of 29412\n","Training classifier...\n","Training samples: 45114 (spam: 22557, clean: 22557)\n","\n","=== MODEL EVALUATION ===\n","[[5618   22]\n"," [  23  220]]\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00      5640\n","           1       0.91      0.91      0.91       243\n","\n","    accuracy                           0.99      5883\n","   macro avg       0.95      0.95      0.95      5883\n","weighted avg       0.99      0.99      0.99      5883\n","\n","Accuracy: 0.9924\n","Detecting duplicates...\n","Duplicates found: 1938 (6.6%)\n","Detecting off-topic reviews...\n","Off-topic found: 586 (2.0%)\n","Calculating informativeness...\n","\n","=== ANALYSIS SUMMARY ===\n","Original reviews: 29412\n","Clean reviews: 27052 (92.0%)\n","Filtered out: 2360 (8.0%)\n","  - Spam: 1238 (4.2%)\n","  - Duplicates: 1938 (6.6%)\n","  - Off-topic: 586 (2.0%)\n","\n","Files saved:\n","- Reviews_Clean.csv: 27052 clean reviews\n","- Reviews_Filtered.csv: 2360 filtered reviews\n","- Reviews_Analysis.csv: Complete analysis results\n"]}]},{"cell_type":"markdown","source":["---\n","\n","# ğŸ“Š `Review Analysis\n","\n","The `ImprovedReviewPresentation` class is an end-to-end system for analyzing customer reviews, identifying quality issues (spam, duplicates, off-topic), and generating both static and interactive dashboards for reporting. It is especially useful in customer experience analysis and content moderation use cases.\n","\n","---\n","\n","## ğŸ§± Key Components\n","\n","### 1. **Initialization**\n","\n","```python\n","analyzer = ImprovedReviewPresentation()\n","```\n","\n","* Loads the datasets\n","* Ensures required columns exist\n","* Creates sample data if input is missing\n","\n","---\n","\n","### 2. **Validation & Cleaning**\n","\n","* Ensures key columns exist (`is_spam`, `spam_probability`, etc.)\n","* Converts columns to appropriate data types\n","* Adds default values if necessary\n","\n","---\n","\n","### 3. **Metric Calculation**\n","\n","Calculates:\n","\n","* Clean vs. filtered review counts\n","* Spam, duplicate, and off-topic counts\n","* Informativeness score (average before and after filtering)\n","* Retention rate and quality improvement %\n","\n","---\n","\n","### 4. **Executive Summary**\n","\n","Outputs a concise text-based report covering:\n","\n","* Filtering performance\n","* Issue breakdown\n","* Quality metrics\n","* Estimated reviewer time saved\n","\n","```python\n","analyzer.print_executive_summary()\n","```\n","\n","---\n","\n","### 5. **Static Dashboard**\n","\n","Generates a 6-panel dashboard using `matplotlib`:\n","\n","* Pie chart: Clean vs Filtered\n","* Bar chart: Issue categories\n","* Bar chart: Metric improvement\n","* Histogram: Spam probability\n","* Histogram: Informativeness (clean vs all)\n","* Histogram: Review length distribution\n","\n","```python\n","analyzer.create_main_dashboard()\n","```\n","\n","---\n","\n","### 6. **Interactive Dashboard**\n","\n","Interactive Plotly dashboard with:\n","\n","* Pie chart, bar charts, scatter plots, gauge\n","* Score histograms and filtering visuals\n","* HTML output for web-based demo or embed\n","\n","```python\n","analyzer.create_interactive_plotly_dashboard()\n","```\n","\n","---\n","\n","### 7. **Detailed Examples**\n","\n","Displays top clean and spam reviews for manual review or presentations.\n","\n","```python\n","analyzer.show_detailed_examples()\n","```\n","\n","---\n"],"metadata":{"id":"nYCPEjmviVzI"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","class ImprovedReviewPresentation:\n","    def __init__(self, analysis_results_path='Reviews_Analysis.csv',\n","                 clean_reviews_path='Reviews_Clean.csv',\n","                 filtered_reviews_path='Reviews_Filtered.csv'):\n","        \"\"\"Initialize presentation analysis with robust error handling.\"\"\"\n","        print(\"ğŸ”„ Loading review analysis data...\")\n","\n","        # Load main analysis results\n","        self.results_df = self._safe_load_csv(analysis_results_path, \"analysis results\")\n","        self.clean_df = self._safe_load_csv(clean_reviews_path, \"clean reviews\")\n","        self.filtered_df = self._safe_load_csv(filtered_reviews_path, \"filtered reviews\")\n","\n","        # Validate and fix data\n","        self._validate_data()\n","\n","        # Calculate metrics\n","        self._calculate_key_metrics()\n","\n","        print(f\"âœ… Data loaded successfully: {len(self.results_df)} total reviews\")\n","\n","    def _safe_load_csv(self, path, description):\n","        \"\"\"Safely load CSV with error handling.\"\"\"\n","        try:\n","            df = pd.read_csv(path)\n","            print(f\"  âœ“ Loaded {description}: {len(df)} records\")\n","            return df\n","        except FileNotFoundError:\n","            print(f\"  âš ï¸ {description.title()} file not found: {path}\")\n","            return pd.DataFrame()\n","        except Exception as e:\n","            print(f\"  âŒ Error loading {description}: {e}\")\n","            return pd.DataFrame()\n","\n","    def _validate_data(self):\n","        \"\"\"Validate and fix data structure.\"\"\"\n","        print(\"ğŸ”§ Validating data structure...\")\n","\n","        # If main results are empty, create sample data\n","        if self.results_df.empty:\n","            print(\"  ğŸ“ Creating sample data for demonstration...\")\n","            self.results_df = self._create_sample_data()\n","\n","        # Ensure required columns exist\n","        required_columns = {\n","            'is_spam': False,\n","            'is_duplicate': False,\n","            'is_off_topic': False,\n","            'spam_probability': 0.1,\n","            'informativeness_score': 0.5,\n","            'review': 'Sample review text'\n","        }\n","\n","        for col, default_val in required_columns.items():\n","            if col not in self.results_df.columns:\n","                print(f\"  + Adding missing column: {col}\")\n","                self.results_df[col] = default_val\n","\n","        # Fix data types\n","        bool_cols = ['is_spam', 'is_duplicate', 'is_off_topic']\n","        for col in bool_cols:\n","            self.results_df[col] = self.results_df[col].astype(bool)\n","\n","        numeric_cols = ['spam_probability', 'informativeness_score']\n","        for col in numeric_cols:\n","            self.results_df[col] = pd.to_numeric(self.results_df[col], errors='coerce').fillna(0.5)\n","\n","        # Ensure review text exists\n","        self.results_df['review'] = self.results_df['review'].astype(str)\n","\n","        print(\"  âœ… Data validation complete\")\n","\n","    def _create_sample_data(self):\n","        \"\"\"Create realistic sample data for demonstration.\"\"\"\n","        n = 1000\n","        np.random.seed(42)  # For reproducible results\n","\n","        return pd.DataFrame({\n","            'review_id': range(n),\n","            'review': [f\"This is sample review {i} with some content about products and services.\"\n","                      for i in range(n)],\n","            'is_spam': np.random.choice([True, False], n, p=[0.05, 0.95]),\n","            'spam_probability': np.random.beta(1, 9, n),\n","            'is_duplicate': np.random.choice([True, False], n, p=[0.08, 0.92]),\n","            'is_off_topic': np.random.choice([True, False], n, p=[0.12, 0.88]),\n","            'informativeness_score': np.random.beta(2, 2, n)\n","        })\n","\n","    def _calculate_key_metrics(self):\n","        \"\"\"Calculate key presentation metrics.\"\"\"\n","        print(\"ğŸ“Š Calculating key metrics...\")\n","\n","        # Calculate clean mask (reviews not flagged for any issue)\n","        self.clean_mask = ~(\n","            self.results_df['is_spam'] |\n","            self.results_df['is_duplicate'] |\n","            self.results_df['is_off_topic']\n","        )\n","\n","        # Core metrics\n","        self.total_reviews = len(self.results_df)\n","        self.clean_count = self.clean_mask.sum()\n","        self.filtered_count = self.total_reviews - self.clean_count\n","\n","        # Issue counts\n","        self.spam_count = self.results_df['is_spam'].sum()\n","        self.duplicate_count = self.results_df['is_duplicate'].sum()\n","        self.off_topic_count = self.results_df['is_off_topic'].sum()\n","\n","        # Quality metrics\n","        self.avg_info_all = self.results_df['informativeness_score'].mean()\n","        self.avg_info_clean = (self.results_df[self.clean_mask]['informativeness_score'].mean()\n","                              if self.clean_count > 0 else self.avg_info_all)\n","\n","        self.retention_rate = (self.clean_count / self.total_reviews) * 100\n","        self.quality_improvement = ((self.avg_info_clean - self.avg_info_all) / self.avg_info_all * 100\n","                                   if self.avg_info_all > 0 else 0)\n","\n","        print(f\"  âœ“ Clean reviews: {self.clean_count:,} ({self.retention_rate:.1f}%)\")\n","        print(f\"  âœ“ Quality improvement: {self.quality_improvement:.1f}%\")\n","\n","    def print_executive_summary(self):\n","        \"\"\"Print comprehensive executive summary.\"\"\"\n","        print(\"\\n\" + \"=\"*70)\n","        print(\"ğŸ“‹ EXECUTIVE SUMMARY - REVIEW FILTERING ANALYSIS\")\n","        print(\"=\"*70)\n","\n","        print(f\"ğŸ“ˆ OVERALL PERFORMANCE:\")\n","        print(f\"  Total Reviews Processed:    {self.total_reviews:,}\")\n","        print(f\"  Clean Reviews Retained:     {self.clean_count:,} ({self.retention_rate:.1f}%)\")\n","        print(f\"  Reviews Filtered Out:       {self.filtered_count:,} ({(self.filtered_count/self.total_reviews)*100:.1f}%)\")\n","\n","        print(f\"\\nğŸ¯ FILTERING BREAKDOWN:\")\n","        print(f\"  Spam Detected:              {self.spam_count:,} ({(self.spam_count/self.total_reviews)*100:.1f}%)\")\n","        print(f\"  Duplicates Found:           {self.duplicate_count:,} ({(self.duplicate_count/self.total_reviews)*100:.1f}%)\")\n","        print(f\"  Off-topic Reviews:          {self.off_topic_count:,} ({(self.off_topic_count/self.total_reviews)*100:.1f}%)\")\n","\n","        print(f\"\\nğŸ“Š QUALITY IMPROVEMENTS:\")\n","        print(f\"  Informativeness Increase:   {self.quality_improvement:.1f}%\")\n","        print(f\"  Average Score (All):        {self.avg_info_all:.3f}\")\n","        print(f\"  Average Score (Clean):      {self.avg_info_clean:.3f}\")\n","\n","        # ROI calculation\n","        time_saved = self.filtered_count * 0.5  # Assume 30 seconds saved per filtered review\n","        print(f\"\\nğŸ’° ESTIMATED IMPACT:\")\n","        print(f\"  Time Saved (hours):         {time_saved/60:.1f}\")\n","        print(f\"  Reviews Requiring Review:   {self.clean_count:,} (down from {self.total_reviews:,})\")\n","\n","        print(\"=\"*70)\n","\n","    def create_main_dashboard(self, save_path='review_analysis_dashboard.png'):\n","        \"\"\"Create comprehensive main dashboard.\"\"\"\n","        print(\"ğŸ¨ Creating main analysis dashboard...\")\n","\n","        fig = plt.figure(figsize=(20, 12))\n","        gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n","\n","        # Main title\n","        fig.suptitle('Review Filtering Analysis Dashboard', fontsize=24, fontweight='bold', y=0.95)\n","\n","        # 1. Overall distribution (large pie chart)\n","        ax1 = fig.add_subplot(gs[0, :2])\n","        if self.filtered_count > 0:\n","            sizes = [self.clean_count, self.filtered_count]\n","            labels = [f'Clean Reviews\\n({self.clean_count:,})', f'Filtered Reviews\\n({self.filtered_count:,})']\n","            colors = ['#27ae60', '#e74c3c']\n","            explode = (0.05, 0.05)\n","\n","            wedges, texts, autotexts = ax1.pie(sizes, labels=labels, colors=colors, explode=explode,\n","                                              autopct='%1.1f%%', startangle=90, textprops={'fontsize': 12})\n","            ax1.set_title('Overall Review Distribution', fontsize=16, fontweight='bold', pad=20)\n","        else:\n","            ax1.text(0.5, 0.5, f'All {self.clean_count:,}\\nReviews are Clean!',\n","                    ha='center', va='center', fontsize=16, fontweight='bold')\n","            ax1.set_title('Perfect Quality!', fontsize=16, fontweight='bold')\n","\n","        # 2. Issues breakdown (bar chart)\n","        ax2 = fig.add_subplot(gs[0, 2:])\n","        categories = ['Spam', 'Duplicates', 'Off-topic']\n","        counts = [self.spam_count, self.duplicate_count, self.off_topic_count]\n","        colors = ['#e74c3c', '#f39c12', '#3498db']\n","\n","        bars = ax2.bar(categories, counts, color=colors, alpha=0.8, edgecolor='black', linewidth=1)\n","        ax2.set_title('Issues Detected by Category', fontsize=14, fontweight='bold')\n","        ax2.set_ylabel('Number of Reviews', fontsize=12)\n","\n","        # Add value labels on bars\n","        for bar, count in zip(bars, counts):\n","            height = bar.get_height()\n","            ax2.text(bar.get_x() + bar.get_width()/2., height + max(height*0.02, 1),\n","                    f'{count:,}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n","\n","        # 3. Quality improvement metrics\n","        ax3 = fig.add_subplot(gs[1, :2])\n","        metrics = ['Retention\\nRate (%)', 'Quality\\nImprovement (%)', 'Avg Info Score\\n(Before)', 'Avg Info Score\\n(After)']\n","        values = [self.retention_rate, self.quality_improvement, self.avg_info_all, self.avg_info_clean]\n","        colors_metrics = ['#27ae60', '#9b59b6', '#34495e', '#2ecc71']\n","\n","        bars_metrics = ax3.bar(metrics, values, color=colors_metrics, alpha=0.8, edgecolor='black')\n","        ax3.set_title('Key Performance Metrics', fontsize=14, fontweight='bold')\n","        ax3.set_ylabel('Score / Percentage', fontsize=12)\n","\n","        for bar, value in zip(bars_metrics, values):\n","            height = bar.get_height()\n","            ax3.text(bar.get_x() + bar.get_width()/2., height + height*0.02,\n","                    f'{value:.1f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n","\n","        # 4. Spam probability distribution\n","        ax4 = fig.add_subplot(gs[1, 2:])\n","        spam_probs = self.results_df['spam_probability']\n","        ax4.hist(spam_probs, bins=30, alpha=0.7, color='#e74c3c', edgecolor='black')\n","        ax4.axvline(spam_probs.mean(), color='darkred', linestyle='--', linewidth=2,\n","                   label=f'Mean: {spam_probs.mean():.3f}')\n","        ax4.set_title('Spam Probability Distribution', fontsize=14, fontweight='bold')\n","        ax4.set_xlabel('Spam Probability', fontsize=12)\n","        ax4.set_ylabel('Frequency', fontsize=12)\n","        ax4.legend()\n","\n","        # 5. Informativeness comparison (bottom row)\n","        ax5 = fig.add_subplot(gs[2, :2])\n","        all_info = self.results_df['informativeness_score']\n","        clean_info = self.results_df[self.clean_mask]['informativeness_score']\n","\n","        ax5.hist(all_info, bins=25, alpha=0.6, label=f'All Reviews (Î¼={all_info.mean():.3f})',\n","                color='lightcoral', density=True, edgecolor='black')\n","        if len(clean_info) > 0:\n","            ax5.hist(clean_info, bins=25, alpha=0.8, label=f'Clean Reviews (Î¼={clean_info.mean():.3f})',\n","                    color='forestgreen', density=True, edgecolor='black')\n","        ax5.set_title('Informativeness Score Distribution', fontsize=14, fontweight='bold')\n","        ax5.set_xlabel('Informativeness Score', fontsize=12)\n","        ax5.set_ylabel('Density', fontsize=12)\n","        ax5.legend()\n","\n","        # 6. Review length analysis\n","        ax6 = fig.add_subplot(gs[2, 2:])\n","        all_lengths = [len(str(review)) for review in self.results_df['review']]\n","        clean_lengths = [len(str(review)) for i, review in enumerate(self.results_df['review'])\n","                        if self.clean_mask.iloc[i]]\n","\n","        ax6.hist(all_lengths, bins=30, alpha=0.6, label=f'All Reviews (Î¼={np.mean(all_lengths):.0f})',\n","                color='lightblue', density=True, range=(0, 500))\n","        if len(clean_lengths) > 0:\n","            ax6.hist(clean_lengths, bins=30, alpha=0.8, label=f'Clean Reviews (Î¼={np.mean(clean_lengths):.0f})',\n","                    color='darkblue', density=True, range=(0, 500))\n","        ax6.set_title('Review Length Distribution', fontsize=14, fontweight='bold')\n","        ax6.set_xlabel('Character Count', fontsize=12)\n","        ax6.set_ylabel('Density', fontsize=12)\n","        ax6.legend()\n","\n","        plt.tight_layout()\n","        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n","        plt.show()\n","\n","        print(f\"  âœ… Dashboard saved as {save_path}\")\n","        return fig\n","\n","    def create_interactive_plotly_dashboard(self, save_path='interactive_dashboard.html'):\n","        \"\"\"Create interactive Plotly dashboard.\"\"\"\n","        print(\"ğŸŒ Creating interactive dashboard...\")\n","\n","        # Create subplots\n","        fig = make_subplots(\n","            rows=2, cols=3,\n","            subplot_titles=('Review Distribution', 'Issues by Category', 'Quality Metrics',\n","                           'Spam vs Informativeness', 'Score Distributions', 'Processing Efficiency'),\n","            specs=[[{\"type\": \"domain\"}, {\"type\": \"xy\"}, {\"type\": \"xy\"}],\n","                   [{\"type\": \"scatter\"}, {\"type\": \"xy\"}, {\"type\": \"indicator\"}]]\n","        )\n","\n","        # 1. Pie chart - Review distribution\n","        fig.add_trace(go.Pie(\n","            labels=['Clean Reviews', 'Filtered Reviews'],\n","            values=[self.clean_count, self.filtered_count],\n","            hole=0.4,\n","            marker_colors=['#27ae60', '#e74c3c'],\n","            textinfo='label+percent+value',\n","            textfont_size=12\n","        ), row=1, col=1)\n","\n","        # 2. Bar chart - Issues by category\n","        fig.add_trace(go.Bar(\n","            x=['Spam', 'Duplicates', 'Off-topic'],\n","            y=[self.spam_count, self.duplicate_count, self.off_topic_count],\n","            marker_color=['#e74c3c', '#f39c12', '#3498db'],\n","            text=[f'{self.spam_count:,}', f'{self.duplicate_count:,}', f'{self.off_topic_count:,}'],\n","            textposition='auto',\n","            name='Issues'\n","        ), row=1, col=2)\n","\n","        # 3. Bar chart - Quality metrics\n","        metrics = ['Retention %', 'Quality Improvement %']\n","        values = [self.retention_rate, self.quality_improvement]\n","        fig.add_trace(go.Bar(\n","            x=metrics,\n","            y=values,\n","            marker_color=['#27ae60', '#9b59b6'],\n","            text=[f'{v:.1f}%' for v in values],\n","            textposition='auto',\n","            name='Metrics'\n","        ), row=1, col=3)\n","\n","        # 4. Scatter plot - Spam probability vs informativeness\n","        colors = ['red' if spam else 'blue' for spam in self.results_df['is_spam']]\n","        fig.add_trace(go.Scatter(\n","            x=self.results_df['spam_probability'],\n","            y=self.results_df['informativeness_score'],\n","            mode='markers',\n","            marker=dict(\n","                color=colors,\n","                size=6,\n","                opacity=0.6,\n","                line=dict(width=1, color='white')\n","            ),\n","            text=[f\"Review {i}<br>Spam: {spam}<br>Info: {info:.3f}\"\n","                  for i, (spam, info) in enumerate(zip(self.results_df['is_spam'],\n","                                                      self.results_df['informativeness_score']))],\n","            hovertemplate='%{text}<extra></extra>',\n","            name='Reviews'\n","        ), row=2, col=1)\n","\n","        # 5. Histogram - Score distributions\n","        fig.add_trace(go.Histogram(\n","            x=self.results_df['informativeness_score'],\n","            name='All Reviews',\n","            opacity=0.6,\n","            nbinsx=20,\n","            marker_color='lightcoral'\n","        ), row=2, col=2)\n","\n","        if self.clean_count > 0:\n","            fig.add_trace(go.Histogram(\n","                x=self.results_df[self.clean_mask]['informativeness_score'],\n","                name='Clean Reviews',\n","                opacity=0.8,\n","                nbinsx=20,\n","                marker_color='forestgreen'\n","            ), row=2, col=2)\n","\n","        # 6. Gauge - Processing efficiency\n","        fig.add_trace(go.Indicator(\n","            mode=\"gauge+number+delta\",\n","            value=self.retention_rate,\n","            delta={'reference': 80},\n","            gauge={\n","                'axis': {'range': [None, 100]},\n","                'bar': {'color': \"#27ae60\"},\n","                'steps': [\n","                    {'range': [0, 50], 'color': \"lightgray\"},\n","                    {'range': [50, 80], 'color': \"gray\"},\n","                    {'range': [80, 100], 'color': \"lightgreen\"}\n","                ],\n","                'threshold': {\n","                    'line': {'color': \"red\", 'width': 4},\n","                    'thickness': 0.75,\n","                    'value': 90\n","                }\n","            },\n","            title={'text': \"Retention Rate (%)\"},\n","            number={'suffix': \"%\"}\n","        ), row=2, col=3)\n","\n","        # Update layout\n","        fig.update_layout(\n","            title_text=\"Interactive Review Filtering Analysis Dashboard\",\n","            title_x=0.5,\n","            title_font_size=20,\n","            showlegend=True,\n","            height=800\n","        )\n","\n","        # Save and show\n","        fig.write_html(save_path)\n","        fig.show()\n","\n","        print(f\"  âœ… Interactive dashboard saved as {save_path}\")\n","        return fig\n","\n","    def show_detailed_examples(self, n_examples=5):\n","        \"\"\"Show detailed examples with context.\"\"\"\n","        print(\"\\n\" + \"=\"*80)\n","        print(\"ğŸ“ DETAILED REVIEW EXAMPLES\")\n","        print(\"=\"*80)\n","\n","        # High-quality clean reviews\n","        if self.clean_count > 0:\n","            print(f\"\\nğŸ† TOP {min(n_examples, self.clean_count)} HIGH-QUALITY CLEAN REVIEWS:\")\n","            print(\"-\" * 70)\n","\n","            clean_examples = (self.results_df[self.clean_mask]\n","                            .nlargest(min(n_examples, self.clean_count), 'informativeness_score'))\n","\n","            for i, (_, row) in enumerate(clean_examples.iterrows(), 1):\n","                print(f\"\\n{i}. CLEAN REVIEW - ID: {row.get('review_id', 'N/A')}\")\n","                print(f\"   ğŸ“Š Informativeness: {row['informativeness_score']:.3f}\")\n","                print(f\"   ğŸš« Spam Probability: {row['spam_probability']:.3f}\")\n","                print(f\"   ğŸ“ Text: \\\"{str(row['review'])[:200]}{'...' if len(str(row['review'])) > 200 else ''}\\\"\")\n","        else:\n","            print(\"\\nğŸ† NO CLEAN REVIEWS FOUND\")\n","\n","        # Spam examples\n","        if self.spam_count > 0:\n","            print(f\"\\nğŸš« TOP {min(n_examples, self.spam_count)} SPAM EXAMPLES:\")\n","            print(\"-\" * 70)\n","\n","            spam_examples = (self.results_df[self.results_df['is_spam']]\n","                           .nlargest(min(n_examples, self.spam_count), 'spam_probability'))\n","\n","            for i, (_, row) in enumerate(spam_examples.iterrows(), 1):\n","                print(f\"\\n{i}. SPAM REVIEW - ID: {row.get('review_id', 'N/A')}\")\n","                print(f\"   ğŸš« Spam Probability: {row['spam_probability']:.3f}\")\n","                print(f\"   ğŸ“Š Informativeness: {row['informativeness_score']:.3f}\")\n","                print(f\"   ğŸ“ Text: \\\"{str(row['review'])[:200]}{'...' if len(str(row['review'])) > 200 else ''}\\\"\")\n","        else:\n","            print(f\"\\nğŸš« NO SPAM EXAMPLES FOUND\")\n","\n","        # Show filtering impact\n","        print(f\"\\nğŸ“ˆ FILTERING IMPACT SUMMARY:\")\n","        print(f\"   â€¢ Processing efficiency: {self.retention_rate:.1f}% of reviews retained\")\n","        print(f\"   â€¢ Quality improvement: {self.quality_improvement:.1f}% increase in informativeness\")\n","        print(f\"   â€¢ Manual review reduction: {self.filtered_count:,} reviews automatically filtered\")\n","\n","    def generate_complete_presentation(self):\n","        \"\"\"Generate all presentation materials.\"\"\"\n","        print(\"\\nğŸ¯ GENERATING COMPLETE PRESENTATION PACKAGE\")\n","        print(\"=\"*70)\n","\n","        try:\n","            # 1. Executive summary\n","            self.print_executive_summary()\n","\n","            # 2. Main dashboard\n","            print(\"\\nğŸ“Š Creating visualizations...\")\n","            self.create_main_dashboard()\n","\n","            # 3. Interactive dashboard\n","            self.create_interactive_plotly_dashboard()\n","\n","            # 4. Detailed examples\n","            self.show_detailed_examples()\n","\n","            print(f\"\\nâœ… PRESENTATION PACKAGE COMPLETE!\")\n","            print(\"=\"*70)\n","            print(\"ğŸ“ Generated Files:\")\n","            print(\"  â€¢ review_analysis_dashboard.png - Main presentation chart\")\n","            print(\"  â€¢ interactive_dashboard.html - Interactive dashboard for demos\")\n","            print(\"\\nğŸ¯ Ready for presentation! Use the PNG for slides and HTML for live demos.\")\n","\n","        except Exception as e:\n","            print(f\"âŒ Error generating presentation: {e}\")\n","            print(\"Some components may not have been created.\")\n","\n","# Simplified usage\n","if __name__ == \"__main__\":\n","    try:\n","        print(\"ğŸš€ STARTING REVIEW ANALYSIS PRESENTATION GENERATOR\")\n","        print(\"=\"*70)\n","\n","        # Initialize analyzer\n","        analyzer = ImprovedReviewPresentation(\n","            analysis_results_path='Reviews_Analysis.csv',\n","            clean_reviews_path='Reviews_Clean.csv',\n","            filtered_reviews_path='Reviews_Filtered.csv'\n","        )\n","\n","        # Generate complete presentation\n","        analyzer.generate_complete_presentation()\n","\n","    except Exception as e:\n","        print(f\"ğŸ’¥ Critical error: {e}\")\n","        print(\"Please check your data files and try again.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1M1cGLrvA2Q8VeW9brpVa6cbV4GZTwUSK"},"id":"3Z7WXEIBdpZR","executionInfo":{"status":"ok","timestamp":1750526646560,"user_tz":-480,"elapsed":6266,"user":{"displayName":"CHENG LE","userId":"15930612432530710348"}},"outputId":"b3276a45-6810-4a6c-9cdb-fff16d82ae42"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":[],"metadata":{"id":"4uHHazLv4LKe"}},{"cell_type":"code","source":[],"metadata":{"id":"15WmjiIY6GvE"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOGkSI2JIr3rYKACT3kDek/"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9e32807844de495eb040d1985680ca26":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4860235e612c4331b3e2d408e6486bee","IPY_MODEL_ee325664fe6945d59915f33e38499ebf","IPY_MODEL_6081529b2ca344348e899ba4b30e1e67"],"layout":"IPY_MODEL_06464759abad4fdd81cdc406f573ac06"}},"4860235e612c4331b3e2d408e6486bee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9611e213a0cb443b99c0ebf8835bcc78","placeholder":"â€‹","style":"IPY_MODEL_b6733cadaee347139a05b5e43bdcd747","value":"Batches:â€‡100%"}},"ee325664fe6945d59915f33e38499ebf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ab1c40569004ca69ca24a32f34f33ef","max":920,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3463e8064f89475a98313d7a56493aa8","value":920}},"6081529b2ca344348e899ba4b30e1e67":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5486e712ee2b48f48886b49a8fca6e35","placeholder":"â€‹","style":"IPY_MODEL_e0422e1097e2428e8d68d28df18f4333","value":"â€‡920/920â€‡[00:15&lt;00:00,â€‡145.88it/s]"}},"06464759abad4fdd81cdc406f573ac06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9611e213a0cb443b99c0ebf8835bcc78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6733cadaee347139a05b5e43bdcd747":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0ab1c40569004ca69ca24a32f34f33ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3463e8064f89475a98313d7a56493aa8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5486e712ee2b48f48886b49a8fca6e35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0422e1097e2428e8d68d28df18f4333":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}