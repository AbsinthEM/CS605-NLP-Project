{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üó∫Ô∏è Scraping Google Maps Reviews for Universal Studios Singapore\n",
        "\n",
        "This notebook demonstrates how to **scrape public Google Maps reviews** for a specific location. The target location in this example is **Universal Studios Singapore**.\n",
        "\n",
        "## üßæ Description\n",
        "\n",
        "The process involves:\n",
        "\n",
        "1. **Setting up Selenium WebDriver** with Chrome.\n",
        "2. **Opening Google Maps** and navigating to the desired location.\n",
        "3. **Automatically clicking on the Reviews section** and scrolling through all available reviews.\n",
        "4. **Expanding full review texts** by clicking \"More\" buttons.\n",
        "5. **Parsing review content** such as reviewer name, rating, review date, and review text using BeautifulSoup.\n",
        "6. **Saving the extracted data** into a CSV file.\n",
        "\n",
        "> ‚ö†Ô∏è Note: This script mimics human interaction and may break if the Google Maps layout or class names change. It is for educational purposes only and should respect website terms of service.\n",
        "\n"
      ],
      "metadata": {
        "id": "8y4Ac_pakFZh"
      },
      "id": "8y4Ac_pakFZh"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mwNJcd3tj95W"
      },
      "id": "mwNJcd3tj95W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aec08a0-9dd0-4cd6-bea6-ca1bcd433e68",
      "metadata": {
        "id": "4aec08a0-9dd0-4cd6-bea6-ca1bcd433e68",
        "outputId": "a775d047-3eef-4015-8542-0e35f0ca2482"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googlemaps in c:\\users\\whlzc\\anaconda3\\lib\\site-packages (4.10.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.20.0 in c:\\users\\whlzc\\anaconda3\\lib\\site-packages (from googlemaps) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\whlzc\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.20.0->googlemaps) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\whlzc\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.20.0->googlemaps) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\whlzc\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.20.0->googlemaps) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\whlzc\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.20.0->googlemaps) (2025.1.31)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install googlemaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66023172-98ab-40ca-9c24-97b8378f22d1",
      "metadata": {
        "id": "66023172-98ab-40ca-9c24-97b8378f22d1",
        "outputId": "780912b3-cc31-475e-8b15-2d274dced566"
      },
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "Enter a place to search:  Singapore Management University\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Found place: Singapore Management University\n",
            "Fetching reviews...\n",
            "\n",
            "Review 1:\n",
            "Author: Skye Cai\n",
            "Rating: 5\n",
            "Text: Big school campus within the city area.\n",
            "The buildings are very modernised.\n",
            "Linkages and underpasses in between the buildings.\n",
            "Also shops and student owned cafe inside the campus.\n",
            "Lots of benches for sitting. Seats with plugs are usually located inside the underpass.\n",
            "--------------------------------------------------\n",
            "Review 2:\n",
            "Author: MC\n",
            "Rating: 2\n",
            "Text: Applied for Jan 25 masters in late Sept 2024, no follow ups from the admission office till late Dec 2024. Upon request for status update, then rushed to arrange for interview but I've already accepted NTU offer. Spent the time to get referral and $100 application fee, felt my time is wasted.\n",
            "--------------------------------------------------\n",
            "Review 3:\n",
            "Author: Gold Paper\n",
            "Rating: 1\n",
            "Text: Review is for SMU Academy. Seems like this is a big issue from other reviews. Expensive courses and website doesn't work to register and make payment. Calling, emailing and the chatbot useless in giving any help. You shouldn't offer the short term courses if you are not interested to do a good job.\n",
            "--------------------------------------------------\n",
            "Review 4:\n",
            "Author: Pierre\n",
            "Rating: 1\n",
            "Text: They gave full scholarship to many international students, but did not give any scholarship to needy local students who are from a medium or low-income family. Many international students got a very good grade in their home country because their A level is much easier than Singapore A level.\n",
            "--------------------------------------------------\n",
            "Review 5:\n",
            "Author: Sandra\n",
            "Rating: 5\n",
            "Text: Besides being a school compound, the space is often used for large scale events too! Was there just last week for the Singapore Night Festival, super cool event!\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import googlemaps\n",
        "import time\n",
        "\n",
        "# Initialize your Google Maps client\n",
        "API_KEY = 'AIzaSyBAsT4jmKx4KTVojVaLNU7JwsgtYLR0FG4'  # replace with your API key\n",
        "gmaps = googlemaps.Client(key=API_KEY)\n",
        "\n",
        "def search_place(keyword):\n",
        "    result = gmaps.places(query=keyword)\n",
        "    if result['results']:\n",
        "        place_id = result['results'][0]['place_id']\n",
        "        name = result['results'][0]['name']\n",
        "        return place_id, name\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def get_reviews(place_id):\n",
        "    details = gmaps.place(place_id=place_id)\n",
        "    reviews = details.get('result', {}).get('reviews', [])\n",
        "    return reviews\n",
        "\n",
        "def main():\n",
        "    keyword = input(\"Enter a place to search: \")\n",
        "    place_id, name = search_place(keyword)\n",
        "\n",
        "    if place_id:\n",
        "        print(f\"\\nFound place: {name}\\nFetching reviews...\\n\")\n",
        "        reviews = get_reviews(place_id)\n",
        "        for idx, review in enumerate(reviews, 1):\n",
        "            print(f\"Review {idx}:\")\n",
        "            print(f\"Author: {review.get('author_name')}\")\n",
        "            print(f\"Rating: {review.get('rating')}\")\n",
        "            print(f\"Text: {review.get('text')}\")\n",
        "            print(\"-\" * 50)\n",
        "    else:\n",
        "        print(\"No place found with that keyword.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "431ca3b0-d6a6-4308-90a7-3e57c742eb27",
      "metadata": {
        "id": "431ca3b0-d6a6-4308-90a7-3e57c742eb27",
        "outputId": "9c0467db-7c17-4f2a-ef26-6a2036ffcb75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "  Obtaining dependency information for selenium from https://files.pythonhosted.org/packages/ea/37/d07ed9d13e571b2115d4ed6956d156c66816ceec0b03b2e463e80d09f572/selenium-4.32.0-py3-none-any.whl.metadata\n",
            "  Downloading selenium-4.32.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\whlzc\\anaconda3\\lib\\site-packages (from selenium) (2.1.0)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Obtaining dependency information for trio~=0.17 from https://files.pythonhosted.org/packages/69/8e/3f6dfda475ecd940e786defe6df6c500734e686c9cd0a0f8ef6821e9b2f2/trio-0.30.0-py3-none-any.whl.metadata\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Obtaining dependency information for trio-websocket~=0.9 from https://files.pythonhosted.org/packages/c7/19/eb640a397bba49ba49ef9dbe2e7e5c04202ba045b6ce2ec36e9cadc51e04/trio_websocket-0.12.2-py3-none-any.whl.metadata\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\whlzc\\anaconda3\\lib\\site-packages (from selenium) (2024.2.2)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\whlzc\\anaconda3\\lib\\site-packages (from selenium) (4.9.0)\n",
            "Collecting websocket-client~=1.8 (from selenium)\n",
            "  Obtaining dependency information for websocket-client~=1.8 from https://files.pythonhosted.org/packages/5a/84/44687a29792a70e111c5c477230a72c4b957d88d16141199bf9acb7537a3/websocket_client-1.8.0-py3-none-any.whl.metadata\n",
            "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
            "  Obtaining dependency information for attrs>=23.2.0 from https://files.pythonhosted.org/packages/77/06/bb80f5f86020c4551da315d78b3ab75e8228f89f0162f2c3a819e407941a/attrs-25.3.0-py3-none-any.whl.metadata\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
            "  Obtaining dependency information for sortedcontainers from https://files.pythonhosted.org/packages/32/46/9cb0e58b2deb7f82b84065f37f3bffeb12413f947f9388e4cac22c4621ce/sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: idna in c:\\users\\whlzc\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.7)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Obtaining dependency information for outcome from https://files.pythonhosted.org/packages/55/8b/5ab7257531a5d830fc8000c476e63c935488d74609b50f9384a643ec0a62/outcome-1.3.0.post0-py2.py3-none-any.whl.metadata\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\whlzc\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: cffi>=1.14 in c:\\users\\whlzc\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Obtaining dependency information for wsproto>=0.14 from https://files.pythonhosted.org/packages/78/58/e860788190eba3bcce367f74d29c4675466ce8dddfba85f7827588416f01/wsproto-1.2.0-py3-none-any.whl.metadata\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\whlzc\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: pycparser in c:\\users\\whlzc\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Obtaining dependency information for h11<1,>=0.9.0 from https://files.pythonhosted.org/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl.metadata\n",
            "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Downloading selenium-4.32.0-py3-none-any.whl (9.4 MB)\n",
            "   ---------------------------------------- 0.0/9.4 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.3/9.4 MB 9.9 MB/s eta 0:00:01\n",
            "   --- ------------------------------------ 0.8/9.4 MB 10.8 MB/s eta 0:00:01\n",
            "   ----- ---------------------------------- 1.2/9.4 MB 11.4 MB/s eta 0:00:01\n",
            "   ------- -------------------------------- 1.7/9.4 MB 10.7 MB/s eta 0:00:01\n",
            "   -------- ------------------------------- 2.0/9.4 MB 9.8 MB/s eta 0:00:01\n",
            "   -------- ------------------------------- 2.0/9.4 MB 10.0 MB/s eta 0:00:01\n",
            "   --------- ------------------------------ 2.2/9.4 MB 7.3 MB/s eta 0:00:01\n",
            "   ---------- ----------------------------- 2.4/9.4 MB 7.4 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 2.8/9.4 MB 7.5 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 3.1/9.4 MB 7.2 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 3.2/9.4 MB 6.8 MB/s eta 0:00:01\n",
            "   -------------- ------------------------- 3.3/9.4 MB 6.6 MB/s eta 0:00:01\n",
            "   -------------- ------------------------- 3.4/9.4 MB 6.0 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 3.6/9.4 MB 5.9 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 3.7/9.4 MB 5.7 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 3.8/9.4 MB 5.5 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 4.0/9.4 MB 5.4 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 4.1/9.4 MB 5.2 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 4.2/9.4 MB 5.1 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 4.6/9.4 MB 5.1 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 4.8/9.4 MB 5.1 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 4.8/9.4 MB 5.0 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 5.0/9.4 MB 4.9 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 5.2/9.4 MB 4.9 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 5.3/9.4 MB 4.9 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 5.4/9.4 MB 4.7 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 5.6/9.4 MB 4.6 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 5.7/9.4 MB 4.6 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 5.7/9.4 MB 4.4 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 5.9/9.4 MB 4.4 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 6.2/9.4 MB 4.4 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 6.5/9.4 MB 4.5 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 6.7/9.4 MB 4.5 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 6.7/9.4 MB 4.4 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 6.9/9.4 MB 4.4 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 7.2/9.4 MB 4.5 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 7.6/9.4 MB 4.6 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 7.7/9.4 MB 4.6 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 7.8/9.4 MB 4.4 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 8.1/9.4 MB 4.5 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 8.4/9.4 MB 4.6 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 8.5/9.4 MB 4.5 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 8.6/9.4 MB 4.4 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 8.8/9.4 MB 4.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 9.1/9.4 MB 4.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  9.3/9.4 MB 4.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 9.4/9.4 MB 4.4 MB/s eta 0:00:00\n",
            "Downloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "   ---------------------------------------- 0.0/499.2 kB ? eta -:--:--\n",
            "   ---------------- ----------------------- 204.8/499.2 kB 6.3 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 358.4/499.2 kB 5.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  491.5/499.2 kB 3.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 499.2/499.2 kB 3.5 MB/s eta 0:00:00\n",
            "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
            "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "   ---------------------------------------- 0.0/63.8 kB ? eta -:--:--\n",
            "   ---------------------------------------- 63.8/63.8 kB ? eta 0:00:00\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: sortedcontainers, websocket-client, h11, attrs, wsproto, outcome, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: websocket-client\n",
            "    Found existing installation: websocket-client 0.58.0\n",
            "    Uninstalling websocket-client-0.58.0:\n",
            "      Successfully uninstalled websocket-client-0.58.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.1.0\n",
            "    Uninstalling attrs-23.1.0:\n",
            "      Successfully uninstalled attrs-23.1.0\n",
            "Successfully installed attrs-25.3.0 h11-0.16.0 outcome-1.3.0.post0 selenium-4.32.0 sortedcontainers-2.4.0 trio-0.30.0 trio-websocket-0.12.2 websocket-client-1.8.0 wsproto-1.2.0\n",
            "Collecting webdriver-manager\n",
            "  Obtaining dependency information for webdriver-manager from https://files.pythonhosted.org/packages/b5/b5/3bd0b038d80950ec13e6a2c8d03ed8354867dc60064b172f2f4ffac8afbe/webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests in c:\\users\\whlzc\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.31.0)\n",
            "Requirement already satisfied: python-dotenv in c:\\users\\whlzc\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\whlzc\\anaconda3\\lib\\site-packages (from webdriver-manager) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\whlzc\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\whlzc\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\whlzc\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\whlzc\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2024.2.2)\n",
            "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Installing collected packages: webdriver-manager\n",
            "Successfully installed webdriver-manager-4.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium\n",
        "!pip install webdriver-manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a80e367-69fd-41a9-baf7-874b7fc3db69",
      "metadata": {
        "scrolled": true,
        "id": "7a80e367-69fd-41a9-baf7-874b7fc3db69",
        "outputId": "7c8f7f1b-39c1-4b80-d858-b02ae3c97258"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Found 360 reviews for Universal Studios Singapore.\n",
            "‚úÖ Reviews saved to google_reviews_uss1.csv\n"
          ]
        }
      ],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "import time\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "place_name = \"Universal Studios Singapore\"\n",
        "output_file = \"google_reviews_uss1.csv\"\n",
        "\n",
        "# --- SETUP SELENIUM WEBDRIVER ---\n",
        "options = Options()\n",
        "options.add_argument(\"--start-maximized\")\n",
        "# options.add_argument(\"--headless\")  # Uncomment if you want to run without opening a browser window\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
        "\n",
        "# --- OPEN GOOGLE MAPS AND SEARCH PLACE ---\n",
        "maps_url = f\"https://www.google.com/maps/search/{place_name.replace(' ', '+')}/?hl=en\"\n",
        "driver.get(maps_url)\n",
        "time.sleep(3)\n",
        "\n",
        "# --- ACCEPT COOKIES IF PROMPTED ---\n",
        "try:\n",
        "    buttons = driver.find_elements(By.TAG_NAME, \"button\")\n",
        "    for btn in buttons:\n",
        "        if btn.text.strip().lower() == \"accept all\":\n",
        "            btn.click()\n",
        "            time.sleep(2)\n",
        "            break\n",
        "except Exception as e:\n",
        "    print(\"No cookie pop-up or already accepted.\")\n",
        "\n",
        "# --- CLICK ON REVIEWS BUTTON ---\n",
        "try:\n",
        "    review_button = None\n",
        "    all_buttons = driver.find_elements(By.TAG_NAME, \"button\")\n",
        "    for btn in all_buttons:\n",
        "        if \"review\" in btn.text.lower():\n",
        "            review_button = btn\n",
        "            break\n",
        "    if review_button:\n",
        "        review_button.click()\n",
        "    else:\n",
        "        driver.find_element(By.PARTIAL_LINK_TEXT, \"Reviews\").click()\n",
        "    time.sleep(3)\n",
        "except Exception as e:\n",
        "    print(\"Failed to find or click Reviews button:\", e)\n",
        "    driver.quit()\n",
        "    exit()\n",
        "\n",
        "# --- SCROLL TO LOAD ALL REVIEWS ---\n",
        "scrollable_div = driver.find_element(By.XPATH, \"//div[contains(@class, 'm6QErb') and contains(@class, 'DxyBCb') and contains(@class, 'kA9KIf') and contains(@class, 'dS8AEf')]\")\n",
        "\n",
        "prev_review_count = 0\n",
        "while True:\n",
        "    driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)\n",
        "    time.sleep(2)\n",
        "    review_elems = driver.find_elements(By.CLASS_NAME, \"jftiEf\")\n",
        "    curr_review_count = len(review_elems)\n",
        "    if curr_review_count == prev_review_count:\n",
        "        break\n",
        "    prev_review_count = curr_review_count\n",
        "\n",
        "# --- CLICK \"MORE\" BUTTONS TO EXPAND REVIEWS ---\n",
        "try:\n",
        "    more_buttons = driver.find_elements(By.XPATH, \"//button[.='More']\")\n",
        "    for btn in more_buttons:\n",
        "        try:\n",
        "            btn.click()\n",
        "            time.sleep(0.5)\n",
        "        except Exception:\n",
        "            continue\n",
        "except Exception as e:\n",
        "    print(\"Error clicking More buttons:\", e)\n",
        "\n",
        "# --- PARSE THE PAGE CONTENT ---\n",
        "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "reviews_data = []\n",
        "\n",
        "review_blocks = soup.find_all('div', {'class': 'jftiEf'})\n",
        "for block in review_blocks:\n",
        "    name_tag = block.find('div', {'class': 'd4r55'})\n",
        "    if not name_tag:\n",
        "        name_tag = block.find('a')\n",
        "    reviewer_name = name_tag.text.strip() if name_tag else \"\"\n",
        "\n",
        "    rating = \"\"\n",
        "    rating_tag = block.find('span', {'aria-label': lambda x: x and 'star' in x})\n",
        "    if rating_tag:\n",
        "        try:\n",
        "            rating = float(rating_tag['aria-label'].split()[0])\n",
        "        except:\n",
        "            rating = rating_tag['aria-label']\n",
        "\n",
        "    date_text = \"\"\n",
        "    date_tag = block.find('span', {'class': 'rsqaWe'})\n",
        "    if date_tag:\n",
        "        date_text = date_tag.text.strip()\n",
        "\n",
        "    review_text = \"\"\n",
        "    content_tag = block.find('span', {'class': 'wiI7pd'})\n",
        "    if not content_tag:\n",
        "        content_tag = block.find('div', {'jsname': 'fk8dgd'})\n",
        "    if content_tag:\n",
        "        review_text = content_tag.text.strip()\n",
        "\n",
        "    reviews_data.append({\n",
        "        \"name\": reviewer_name,\n",
        "        \"rating\": rating,\n",
        "        \"date\": date_text,\n",
        "        \"text\": review_text\n",
        "    })\n",
        "\n",
        "print(f\"‚úÖ Found {len(reviews_data)} reviews for {place_name}.\")\n",
        "\n",
        "# --- SAVE TO CSV ---\n",
        "fieldnames = [\"name\", \"rating\", \"date\", \"text\"]\n",
        "with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for review in reviews_data:\n",
        "        writer.writerow(review)\n",
        "\n",
        "print(f\"‚úÖ Reviews saved to {output_file}\")\n",
        "\n",
        "# --- CLOSE BROWSER ---\n",
        "driver.quit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4fc6d9b-2577-4289-9370-d82c034a1eaa",
      "metadata": {
        "id": "b4fc6d9b-2577-4289-9370-d82c034a1eaa",
        "outputId": "e89ff625-84cc-4ea3-8cc7-28d09f7992d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Scrolled... Reviews loaded: 20\n",
            "üîÑ Scrolled... Reviews loaded: 30\n",
            "üîÑ Scrolled... Reviews loaded: 40\n",
            "üîÑ Scrolled... Reviews loaded: 50\n",
            "üîÑ Scrolled... Reviews loaded: 60\n",
            "üîÑ Scrolled... Reviews loaded: 70\n",
            "üîÑ Scrolled... Reviews loaded: 80\n",
            "üîÑ Scrolled... Reviews loaded: 90\n",
            "üîÑ Scrolled... Reviews loaded: 100\n",
            "üîÑ Scrolled... Reviews loaded: 110\n",
            "üîÑ Scrolled... Reviews loaded: 120\n",
            "üîÑ Scrolled... Reviews loaded: 130\n",
            "üîÑ Scrolled... Reviews loaded: 140\n",
            "üîÑ Scrolled... Reviews loaded: 150\n",
            "üîÑ Scrolled... Reviews loaded: 160\n",
            "üîÑ Scrolled... Reviews loaded: 170\n",
            "üîÑ Scrolled... Reviews loaded: 180\n",
            "üîÑ Scrolled... Reviews loaded: 190\n",
            "üîÑ Scrolled... Reviews loaded: 200\n",
            "üîÑ Scrolled... Reviews loaded: 210\n",
            "üîÑ Scrolled... Reviews loaded: 220\n",
            "üîÑ Scrolled... Reviews loaded: 230\n",
            "üîÑ Scrolled... Reviews loaded: 240\n",
            "üîÑ Scrolled... Reviews loaded: 250\n",
            "üîÑ Scrolled... Reviews loaded: 260\n",
            "üîÑ Scrolled... Reviews loaded: 270\n",
            "üîÑ Scrolled... Reviews loaded: 280\n",
            "üîÑ Scrolled... Reviews loaded: 290\n",
            "üîÑ Scrolled... Reviews loaded: 300\n",
            "üîÑ Scrolled... Reviews loaded: 310\n",
            "üîÑ Scrolled... Reviews loaded: 320\n",
            "üîÑ Scrolled... Reviews loaded: 330\n",
            "üîÑ Scrolled... Reviews loaded: 340\n",
            "üîÑ Scrolled... Reviews loaded: 350\n",
            "üîÑ Scrolled... Reviews loaded: 360\n",
            "üîÑ Scrolled... Reviews loaded: 360\n",
            "üîÑ Scrolled... Reviews loaded: 360\n",
            "üîÑ Scrolled... Reviews loaded: 360\n",
            "üîÑ Scrolled... Reviews loaded: 360\n",
            "üîÑ Scrolled... Reviews loaded: 360\n",
            "üîÑ Scrolled... Reviews loaded: 360\n",
            "üîÑ Scrolled... Reviews loaded: 360\n",
            "üîÑ Scrolled... Reviews loaded: 360\n",
            "üîÑ Scrolled... Reviews loaded: 360\n",
            "üõë No more new reviews after retries. Stopping scroll.\n",
            "‚úÖ Found 360 reviews for Universal Studios Singapore.\n",
            "‚úÖ Reviews saved to google_reviews_uss_full.csv\n"
          ]
        }
      ],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "import time\n",
        "import random\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "place_name = \"Universal Studios Singapore\"\n",
        "output_file = \"google_reviews_uss_full.csv\"\n",
        "\n",
        "# --- SETUP SELENIUM WEBDRIVER ---\n",
        "options = Options()\n",
        "options.add_argument(\"--start-maximized\")\n",
        "# options.add_argument(\"--headless\")  # Uncomment to run headless\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
        "\n",
        "# --- OPEN GOOGLE MAPS AND SEARCH PLACE ---\n",
        "maps_url = f\"https://www.google.com/maps/search/{place_name.replace(' ', '+')}/?hl=en\"\n",
        "driver.get(maps_url)\n",
        "time.sleep(3)\n",
        "\n",
        "# --- ACCEPT COOKIES IF PROMPTED ---\n",
        "try:\n",
        "    buttons = driver.find_elements(By.TAG_NAME, \"button\")\n",
        "    for btn in buttons:\n",
        "        if btn.text.strip().lower() == \"accept all\":\n",
        "            btn.click()\n",
        "            time.sleep(2)\n",
        "            break\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# --- CLICK ON REVIEWS BUTTON ---\n",
        "try:\n",
        "    review_button = None\n",
        "    all_buttons = driver.find_elements(By.TAG_NAME, \"button\")\n",
        "    for btn in all_buttons:\n",
        "        if \"review\" in btn.text.lower():\n",
        "            review_button = btn\n",
        "            break\n",
        "    if review_button:\n",
        "        review_button.click()\n",
        "    else:\n",
        "        driver.find_element(By.PARTIAL_LINK_TEXT, \"Reviews\").click()\n",
        "    time.sleep(3)\n",
        "except Exception as e:\n",
        "    print(\"Failed to click Reviews:\", e)\n",
        "    driver.quit()\n",
        "    exit()\n",
        "\n",
        "# --- SCROLL TO LOAD ALL REVIEWS ---\n",
        "scrollable_div = driver.find_element(By.XPATH, \"//div[contains(@class, 'm6QErb') and contains(@class, 'DxyBCb') and contains(@class, 'kA9KIf') and contains(@class, 'dS8AEf')]\")\n",
        "\n",
        "max_scrolls = 500\n",
        "stall_limit = 10\n",
        "stall_count = 0\n",
        "prev_review_count = 0\n",
        "\n",
        "while max_scrolls > 0:\n",
        "    driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)\n",
        "    time.sleep(random.uniform(1.5, 3.5))  # Human-like wait\n",
        "\n",
        "    review_elems = driver.find_elements(By.CLASS_NAME, \"jftiEf\")\n",
        "    curr_review_count = len(review_elems)\n",
        "\n",
        "    if curr_review_count == prev_review_count:\n",
        "        stall_count += 1\n",
        "        if stall_count >= stall_limit:\n",
        "            print(\"üõë No more new reviews after retries. Stopping scroll.\")\n",
        "            break\n",
        "    else:\n",
        "        stall_count = 0\n",
        "        prev_review_count = curr_review_count\n",
        "\n",
        "    max_scrolls -= 1\n",
        "    print(f\"üîÑ Scrolled... Reviews loaded: {curr_review_count}\")\n",
        "\n",
        "# --- CLICK \"MORE\" BUTTONS TO EXPAND REVIEWS ---\n",
        "try:\n",
        "    more_buttons = driver.find_elements(By.XPATH, \"//button[.='More']\")\n",
        "    for btn in more_buttons:\n",
        "        try:\n",
        "            btn.click()\n",
        "            time.sleep(0.5)\n",
        "        except Exception:\n",
        "            continue\n",
        "except Exception as e:\n",
        "    print(\"Error clicking 'More' buttons:\", e)\n",
        "\n",
        "# --- PARSE THE PAGE CONTENT ---\n",
        "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "reviews_data = []\n",
        "\n",
        "review_blocks = soup.find_all('div', {'class': 'jftiEf'})\n",
        "for block in review_blocks:\n",
        "    name_tag = block.find('div', {'class': 'd4r55'})\n",
        "    if not name_tag:\n",
        "        name_tag = block.find('a')\n",
        "    reviewer_name = name_tag.text.strip() if name_tag else \"\"\n",
        "\n",
        "    rating = \"\"\n",
        "    rating_tag = block.find('span', {'aria-label': lambda x: x and 'star' in x})\n",
        "    if rating_tag:\n",
        "        try:\n",
        "            rating = float(rating_tag['aria-label'].split()[0])\n",
        "        except:\n",
        "            rating = rating_tag['aria-label']\n",
        "\n",
        "    date_text = \"\"\n",
        "    date_tag = block.find('span', {'class': 'rsqaWe'})\n",
        "    if date_tag:\n",
        "        date_text = date_tag.text.strip()\n",
        "\n",
        "    review_text = \"\"\n",
        "    content_tag = block.find('span', {'class': 'wiI7pd'})\n",
        "    if not content_tag:\n",
        "        content_tag = block.find('div', {'jsname': 'fk8dgd'})\n",
        "    if content_tag:\n",
        "        review_text = content_tag.text.strip()\n",
        "\n",
        "    reviews_data.append({\n",
        "        \"name\": reviewer_name,\n",
        "        \"rating\": rating,\n",
        "        \"date\": date_text,\n",
        "        \"text\": review_text\n",
        "    })\n",
        "\n",
        "print(f\"‚úÖ Found {len(reviews_data)} reviews for {place_name}.\")\n",
        "\n",
        "# --- SAVE TO CSV ---\n",
        "fieldnames = [\"name\", \"rating\", \"date\", \"text\"]\n",
        "with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for review in reviews_data:\n",
        "        writer.writerow(review)\n",
        "\n",
        "print(f\"‚úÖ Reviews saved to {output_file}\")\n",
        "\n",
        "# --- CLOSE BROWSER ---\n",
        "driver.quit()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}